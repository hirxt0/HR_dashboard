from metadata_processor import MetadataProcessorRU
from tg_parser import TelegramDatabase
from typing import List, Dict
from tqdm import tqdm
import json


class MessageClassifier:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –ë–î
    """
    
    def __init__(self, db_path: str = "telegram_data.db"):
        self.db = TelegramDatabase(db_path)
        
        print(" –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        self.processor = MetadataProcessorRU()
        print(" –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤!\n")
    
    def process_message(self, message: Dict) -> Dict:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π
        text = message.get('text_cleaned') or message.get('text', '')
        
        if not text or len(text) < 20:
            return self._get_empty_metadata()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏
            tags = self.processor.extract_tags(text, top_n=5)
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_data = self.processor.analyze_sentiment(text)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–º—ã
            topic_analysis = self.processor.classify_topic(text, tags)
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –∏–Ω—Å–∞–π–¥–æ–≤
            insider_data = self.processor.detect_insider(text)
            
            return {
                'tags': tags,
                'sentiment': sentiment_data['sentiment'],
                'sentiment_score': sentiment_data['score'],
                'category': topic_analysis['main_topic'],
                'topic_scores': topic_analysis['scores'],
                'topic_details': topic_analysis.get('details', {}),
                'is_insider': insider_data['is_insider'],
                'insider_confidence': insider_data['confidence']
            }
            
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return self._get_empty_metadata()
    
    def _get_empty_metadata(self) -> Dict:
        """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            'tags': [],
            'sentiment': 'neutral',
            'sentiment_score': 0.0,
            'category': '–æ–±—â–µ–µ',
            'topic_scores': {},
            'topic_details': {},
            'is_insider': False,
            'insider_confidence': 0.0
        }
    
    def process_unprocessed_messages(self, batch_size: int = 50, limit: int = None):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –ë–î
        """
        print("–ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –°–û–û–ë–©–ï–ù–ò–ô")

        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = self.db.get_unprocessed_messages(limit=limit)
        
        if not messages:
            print("‚úÖ –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            return
        
        print(f" –ù–∞–π–¥–µ–Ω–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
        print(f" –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...\n")
        
        processed_count = 0
        error_count = 0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞
        category_stats = {}
        sentiment_stats = {'positive': 0, 'neutral': 0, 'negative': 0}
        all_tags = []
        
        for i in tqdm(range(0, len(messages), batch_size), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞"):
            batch = messages[i:i + batch_size]
            
            for message in batch:
                try:
                    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                    metadata = self.process_message(message)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
                    self.db.insert_metadata(message['id'], metadata)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    processed_count += 1
                    category = metadata['category']
                    category_stats[category] = category_stats.get(category, 0) + 1
                    sentiment_stats[metadata['sentiment']] += 1
                    all_tags.extend(metadata['tags'])
                    
                except Exception as e:
                    error_count += 1
                    print(f"\n –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {message['id']}: {e}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._print_processing_stats(
            processed_count, 
            error_count, 
            category_stats, 
            sentiment_stats, 
            all_tags
        )
    
    def _print_processing_stats(self, processed: int, errors: int, 
                               categories: Dict, sentiments: Dict, tags: List):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        from collections import Counter
        
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        
        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}")
        print(f" –û—à–∏–±–æ–∫: {errors}")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        print(f"\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for cat in sorted(categories.keys(), key=lambda x: categories[x], reverse=True):
            count = categories[cat]
            percentage = (count / processed) * 100 if processed > 0 else 0
            print(f"  ‚Ä¢ {cat:20s}: {count:4d} ({percentage:5.1f}%)")
        
        # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        print(f"\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:")
        for sent, count in sentiments.items():
            percentage = (count / processed) * 100 if processed > 0 else 0
            print(f"  ‚Ä¢ {sent:20s}: {count:4d} ({percentage:5.1f}%)")
        
        # –¢–æ–ø —Ç–µ–≥–∏
        if tags:
            tag_counts = Counter(tags)
            print(f"\n –¢–æ–ø-15 —Ç–µ–≥–æ–≤:")
            for tag, count in tag_counts.most_common(15):
                print(f"  ‚Ä¢ {tag:25s}: {count}")
        
    
    def search_by_tags(self, query_tags: List[str], limit: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ —Ç–µ–≥–∞–º
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        results = self.db.search_by_tags(query_tags, limit=limit)
        
        # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è
        for r in results:
            try:
                r['tags'] = json.loads(r['tags']) if r.get('tags') else []
                r['topic_scores'] = json.loads(r['topic_scores']) if r.get('topic_scores') else {}
            except:
                pass
        
        return results
    
    def get_category_distribution(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT category, COUNT(*) as count
            FROM message_metadata
            WHERE category IS NOT NULL
            GROUP BY category
            ORDER BY count DESC
        ''')
        return {row[0]: row[1] for row in cursor.fetchall()}
    
    def get_sentiment_distribution(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT sentiment, COUNT(*) as count
            FROM message_metadata
            WHERE sentiment IS NOT NULL
            GROUP BY sentiment
            ORDER BY count DESC
        ''')
        return {row[0]: row[1] for row in cursor.fetchall()}
    
    def get_insider_messages(self, min_confidence: float = 0.5, limit: int = 20) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Å–∞–π–¥–µ—Ä—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT m.*, mm.*
            FROM messages m
            JOIN message_metadata mm ON m.id = mm.message_id
            WHERE mm.is_insider = 1 AND mm.insider_confidence >= ?
            ORDER BY mm.insider_confidence DESC
            LIMIT ?
        ''', (min_confidence, limit))
        
        results = [dict(row) for row in cursor.fetchall()]
        
        # –ü–∞—Ä—Å–∏–º JSON
        for r in results:
            try:
                r['tags'] = json.loads(r['tags']) if r.get('tags') else []
            except:
                pass
        
        return results
    
    def export_to_json(self, output_path: str, limit: int = None):
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
        cursor = self.db.conn.cursor()
        
        query = '''
            SELECT m.*, mm.*
            FROM messages m
            JOIN message_metadata mm ON m.id = mm.message_id
            ORDER BY m.datetime DESC
        '''
        
        if limit:
            query += f' LIMIT {limit}'
        
        cursor.execute(query)
        results = [dict(row) for row in cursor.fetchall()]
        
        # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è
        for r in results:
            try:
                r['tags'] = json.loads(r['tags']) if r.get('tags') else []
                r['topic_scores'] = json.loads(r['topic_scores']) if r.get('topic_scores') else {}
            except:
                pass
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f" –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_path}")
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î"""
        self.db.close()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    classifier = MessageClassifier("telegram_data.db")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    classifier.process_unprocessed_messages(batch_size=50, limit=100)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
    for cat, count in classifier.get_category_distribution().items():
        print(f"  ‚Ä¢ {cat}: {count}")
    
    print(f"\n–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:")
    for sent, count in classifier.get_sentiment_distribution().items():
        print(f"  ‚Ä¢ {sent}: {count}")
    
    # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º
    print("\nüîç –ü–û–ò–°–ö –ü–û –¢–ï–ì–ê–ú ['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç']:")
    results = classifier.search_by_tags(['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç'], limit=3)
    for r in results:
        print(f"\n[{r['channel']}] {r['datetime']}")
        print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {r['category']} | –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {r['sentiment']}")
        print(f"–¢–µ–≥–∏: {', '.join(r['tags'][:5])}")
        print(f"{r['text'][:150]}...")
    
    # –ò–Ω—Å–∞–π–¥—ã
    insiders = classifier.get_insider_messages(min_confidence=0.5, limit=5)
    if insiders:
        print(f"\nüîí –ò–ù–°–ê–ô–î–ï–†–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø ({len(insiders)} —Å–æ–æ–±—â–µ–Ω–∏–π):")
        for ins in insiders:
            print(f"\n[{ins['channel']}] –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {ins['insider_confidence']:.0%}")
            print(f"{ins['text'][:150]}...")
    
    # –≠–∫—Å–ø–æ—Ä—Ç
    classifier.export_to_json("classified_messages.json", limit=1000)
    
    classifier.close()