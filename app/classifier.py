# classifier.py
import sqlite3
import json
from typing import List, Dict, Optional, Tuple
import requests
from dataclasses import dataclass
import time
import re
from dotenv import load_dotenv
import os

load_dotenv()
API_KEY = os.getenv("GROQ_API_KEY")

@dataclass
class ChunkData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞"""
    chunk_id: int
    text: str
    existing_metadata: Dict

class LLMMetadataClassifier:
    """
    LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–≥–∏ –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º
    """
    
    # –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ–≥–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞
    PREDEFINED_TAGS = [
        # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–¥–∞–Ω–Ω—ã–µ",
        "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "–æ–±–ª–∞—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "big data", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
        
        # –ë–∏–∑–Ω–µ—Å
        "–±–∏–∑–Ω–µ—Å", "—Å—Ç–∞—Ä—Ç–∞–ø—ã", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—ç–∫–æ–Ω–æ–º–∏–∫–∞",
        "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "–∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è", "—Ä—ã–Ω–æ–∫",
        
        # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "–æ–±—É—á–µ–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "–Ω–∞—É–∫–∞", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç",
        "–∫—É—Ä—Å—ã", "–Ω–∞–≤—ã–∫–∏", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "–∑–Ω–∞–Ω–∏—è", "–∞–∫–∞–¥–µ–º–∏—è",
        
        # –†–∞–∑–Ω–æ–µ
        "–Ω–æ–≤–æ—Å—Ç–∏", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "—Ç—Ä–µ–Ω–¥—ã", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "–±—É–¥—É—â–µ–µ",
        "–∑–¥–æ—Ä–æ–≤—å–µ", "—ç–∫–æ–ª–æ–≥–∏—è", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–∫—É–ª—å—Ç—É—Ä–∞", "—Å–ø–æ—Ä—Ç", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è",
        "–º–µ–¥–∏—Ü–∏–Ω–∞", "—Ä–∞–±–æ—Ç–∞", "–∫–∞—Ä—å–µ—Ä–∞", "–ª–∏–¥–µ—Ä—Å—Ç–≤–æ", "–∫–æ–º–∞–Ω–¥–∞", "–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
        "—Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏", "–º–µ–¥–∏–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–æ", "–º—É–∑—ã–∫–∞", "–∫–∏–Ω–æ", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞"
    ]
    
    def __init__(self, db_path: str = "telegram_data.db", api_key: Optional[str] = None):
        """
        Args:
            db_path: –ø—É—Ç—å –∫ SQLite –±–∞–∑–µ
            api_key: API –∫–ª—é—á –¥–ª—è Groq
        """
        self.db_path = db_path
        self.api_key = api_key
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        self.model = "llama-3.3-70b-versatile"
        
        if db_path and os.path.exists(db_path):
            self._init_db()
        
    def _init_db(self):
        """–∫–æ–Ω–Ω–µ–∫—Ç —Å –±–¥ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã chunks
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='chunks'
            """)
            
            if cursor.fetchone():
                columns_to_add = [
                    ("llm_tags", "TEXT"),
                    ("sentiment", "TEXT"),
                    ("explanation", "TEXT"),  # –í–º–µ—Å—Ç–æ sentiment_score
                ]
                
                for col_name, col_type in columns_to_add:
                    try:
                        cursor.execute(f"""
                            ALTER TABLE chunks ADD COLUMN {col_name} {col_type}
                        """)
                    except sqlite3.OperationalError:
                        pass
                
                conn.commit()
    
    def get_chunks(self, limit: Optional[int] = None) -> List[ChunkData]:
        """–ø–æ–ª—É—á–∞–µ–º –∏–∑ –±–¥ —á–∞–Ω–∫–∏"""
        if not os.path.exists(self.db_path):
            return []
            
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='chunks'
            """)
            
            if not cursor.fetchone():
                return []
            
            query = "SELECT id, text, metadata FROM chunks WHERE llm_tags IS NULL"
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            rows = cursor.fetchall()
            
            chunks = []
            for row in rows:
                chunk_id, text, metadata_str = row
                
                try:
                    existing_metadata = json.loads(metadata_str) if metadata_str else {}
                except:
                    existing_metadata = {}
                
                chunks.append(ChunkData(
                    chunk_id=chunk_id,
                    text=text,
                    existing_metadata=existing_metadata
                ))
            return chunks
    # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –≤ –∫–ª–∞—Å—Å LLMMetadataClassifier –≤ classifier.py

    def generate_recommendations(self, tag: str, signal_type: str, 
                                sentiment_distribution: Dict[str, float],
                                mentions_count: int, trend: str) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞ —Å –ø–æ–º–æ—â—å—é LLM

        Args:
            tag: —Ç–µ–≥ —Å–∏–≥–Ω–∞–ª–∞
            signal_type: —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ (problem, opportunity, early_trend –∏ —Ç.–¥.)
            sentiment_distribution: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
            mentions_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            trend: —Ç—Ä–µ–Ω–¥ (up, down, stable)

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (2-4 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)
        """
        if not self.api_key or self.api_key == "–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å":
            print("–û—à–∏–±–∫–∞: API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
            return ["–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –ø–æ —Ç–µ–≥—É", "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–∏–≥–Ω–∞–ª–∞
        signal_context = {
            'problem': "–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è, —Ç—Ä–µ–±—É—é—â–∞—è —Ä–µ—à–µ–Ω–∏—è",
            'growing_problem': "–Ω–∞—Ä–∞—Å—Ç–∞—é—â–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞",
            'new_problem': "–Ω–æ–≤–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞",
            'opportunity': "–ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è", 
            'growing_opportunity': "—Ä–∞—Å—Ç—É—â–∞—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å",
            'new_opportunity': "–Ω–æ–≤–∞—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å",
            'new_trend': "–Ω–æ–≤—ã–π —Ç—Ä–µ–Ω–¥ –Ω–∞ —Ä—ã–Ω–∫–µ",
            'emerging_trend': "–∑–∞—Ä–æ–∂–¥–∞—é—â–∏–π—Å—è —Ç—Ä–µ–Ω–¥",
            'early_trend': "—Ñ–æ—Ä–º–∏—Ä—É—é—â–∏–π—Å—è —Ç—Ä–µ–Ω–¥"
        }

        context = signal_context.get(signal_type, "—Ç—Ä–µ–Ω–¥ –Ω–∞ —Ä—ã–Ω–∫–µ")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
        prompt = f"""–¢—ã HR-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –∫—Ä—É–ø–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª –∏ –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:

    –¢–ï–ì –°–ò–ì–ù–ê–õ–ê: "{tag}"
    –¢–ò–ü –°–ò–ì–ù–ê–õ–ê: {signal_type} ({context})
    –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ù–ê–°–¢–†–û–ï–ù–ò–ô:
     - –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {sentiment_distribution.get('positive', 0)}%
     - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {sentiment_distribution.get('negative', 0)}%
     - –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: {sentiment_distribution.get('neutral', 0)}%
    –í–°–ï–ì–û –£–ü–û–ú–ò–ù–ê–ù–ò–ô: {mentions_count}
    –¢–†–ï–ù–î: {'—Ä–∞—Å—Ç–µ—Ç' if trend == 'up' else '–ø–∞–¥–∞–µ—Ç' if trend == 'down' else '—Å—Ç–∞–±–∏–ª–µ–Ω'}

    –°–§–ï–†–ê –ê–ù–ê–õ–ò–ó–ê:
    1. HR –∏ –∫–∞–¥—Ä–æ–≤–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ (—Ä–µ–∫—Ä—É—Ç–∏–Ω–≥, –æ–±—É—á–µ–Ω–∏–µ, –º–æ—Ç–∏–≤–∞—Ü–∏—è)
    2. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
    3. –ë–∏–∑–Ω–µ—Å-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è
    4. –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏
    5. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏

    –°–§–û–†–ú–ò–†–£–ô 3-4 –ö–û–ù–ö–†–ï–¢–ù–´–• –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –ö–û–ú–ü–ê–ù–ò–ò.
    –ö–∞–∂–¥–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å:
    - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –∏ –≤—ã–ø–æ–ª–Ω–∏–º–æ–π
    - –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π (—á—Ç–æ –∏–º–µ–Ω–Ω–æ –¥–µ–ª–∞—Ç—å)
    - –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    - –£–º–µ—Å—Ç–Ω–æ–π –¥–ª—è —Ç–∏–ø–∞ —Å–∏–≥–Ω–∞–ª–∞
    - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Ç–∫–æ–π (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)

    –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–°–¢–†–û–ì–û —Å–æ–±–ª—é–¥–∞–π):
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 1: [—Ç–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏]
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 2: [—Ç–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏] 
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 3: [—Ç–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏]
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 4: [—Ç–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏]

    –ü—Ä–∏–º–µ—Ä –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞:
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 1: –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ–≥–∞—Ç–∏–≤–∞ –ø–æ —Ç–µ–º–µ "{tag}"
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 2: –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø–ª–∞–Ω –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 3: –û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–µ—á—É —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–∞–º–∏
    –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø 4: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–µ–º—ã –≤ —Ç–µ—á–µ–Ω–∏–µ –º–µ—Å—è—Ü–∞"""

        try:
            response = requests.post(
                self.api_url,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": """–¢—ã –æ–ø—ã—Ç–Ω—ã–π HR-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. 
                            –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞.
                            –°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π —Ñ–æ—Ä–º–∞—Ç—É –æ—Ç–≤–µ—Ç–∞. –ö–∞–∂–¥–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è - –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.
                            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω—ã–º–∏ –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã."""
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.4,
                    "max_tokens": 300,
                    "stream": False
                },
                timeout=20
            )

            if response.status_code != 200:
                print(f"‚ùå API –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: HTTP {response.status_code}")
                return self._get_default_recommendations(tag, signal_type)

            result = response.json()

            if 'error' in result:
                print(f"‚ùå API –æ—à–∏–±–∫–∞: {result['error'].get('message', 'Unknown error')}")
                return self._get_default_recommendations(tag, signal_type)

            if result.get('choices') and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content'].strip()
                print(f"ü§ñ LLM —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ç–µ–≥–∞ '{tag}':")
                print(content)

                # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                recommendations = self._parse_recommendations(content)

                # –ï—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –º–∞–ª–æ, –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
                if len(recommendations) < 2:
                    recommendations.extend(self._get_default_recommendations(tag, signal_type)[:2])

                # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                unique_recommendations = []
                for rec in recommendations:
                    if rec and rec not in unique_recommendations and len(rec) > 10:
                        unique_recommendations.append(rec)

                return unique_recommendations[:4]  # –ú–∞–∫—Å–∏–º—É–º 4 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

            else:
                return self._get_default_recommendations(tag, signal_type)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return self._get_default_recommendations(tag, signal_type)

    def _parse_recommendations(self, content: str) -> List[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        recommendations = []

        # –ò—â–µ–º –≤—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø X: —Ç–µ–∫—Å—Ç"
        pattern = r'–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø\s*\d+\s*:\s*(.+?)(?:\n|$|–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø)'
        matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)

        for match in matches:
            rec = match.strip()
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            rec = re.sub(r'^\W+|\W+$', '', rec)
            if rec and len(rec) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                recommendations.append(rec)

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∏—â–µ–º —Å–ø–∏—Å–∫–æ–º
        if not recommendations:
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                if (line and 
                    not line.startswith('–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø') and
                    not line.startswith('–¢–ï–ì') and
                    not line.startswith('–¢–ò–ü') and
                    len(line) > 10):
                    # –£–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–∞
                    line = re.sub(r'^[‚Ä¢\-\d\.\)]\s*', '', line)
                    recommendations.append(line)

        return recommendations

    def _get_default_recommendations(self, tag: str, signal_type: str) -> List[str]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –µ—Å–ª–∏ LLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª"""
        default_recommendations = {
            'problem': [
                f"–ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω –Ω–µ–≥–∞—Ç–∏–≤–∞ –ø–æ —Ç–µ–º–µ '{tag}'",
                f"–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø–ª–∞–Ω –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–µ '{tag}'",
                f"–û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–µ—á—É —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–∞–º–∏",
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–µ–º—ã"
            ],
            'growing_problem': [
                f"–°–†–û–ß–ù–û –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –ø–æ —Ç–µ–º–µ '{tag}'",
                f"–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∫—Ä–∏–∑–∏—Å–Ω—ã–π –ø–ª–∞–Ω –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏",
                f"–°–æ–∑–¥–∞—Ç—å —Ä–∞–±–æ—á—É—é –≥—Ä—É–ø–ø—É –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã",
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Ç—É–∞—Ü–∏–∏"
            ],
            'opportunity': [
                f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ç–µ–º—ã '{tag}' –¥–ª—è –±–∏–∑–Ω–µ—Å–∞",
                f"–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π",
                f"–ò–∑—É—á–∏—Ç—å —É—Å–ø–µ—à–Ω—ã–µ –∫–µ–π—Å—ã –ø–æ —Ç–µ–º–µ '{tag}'",
                f"–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –¥–∞–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"
            ],
            'growing_opportunity': [
                f"–ê–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–Ω–¥ –ø–æ —Ç–µ–º–µ '{tag}'",
                f"–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
                f"–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—É—é –∫–∞–º–ø–∞–Ω–∏—é",
                f"–ò–∑—É—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞ –≤ —ç—Ç–æ–π —Å—Ñ–µ—Ä–µ"
            ],
            'new_trend': [
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞ '{tag}'",
                f"–ò–∑—É—á–∏—Ç—å —Ä–∞–Ω–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç—Ä–µ–Ω–¥–∞",
                f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –±–∏–∑–Ω–µ—Å–∞",
                f"–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è"
            ]
        }

        # –ò—â–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if signal_type in default_recommendations:
            return default_recommendations[signal_type]
        elif 'problem' in signal_type:
            return default_recommendations['problem']
        elif 'opportunity' in signal_type:
            return default_recommendations['opportunity']
        else:
            return [
                f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–Ω–¥ '{tag}'",
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç–µ–º—ã",
                f"–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π",
                f"–û—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –±–∏–∑–Ω–µ—Å"
            ]
    def analyze_with_llm(self, text: str) -> Tuple[List[str], str, str]:
            """
            –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–µ–≥–∏ (–≤—ã–±–∏—Ä–∞–µ—Ç –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞) –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
            —Å –∫—Ä–∞—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º –≤—ã–±–æ—Ä–∞ —Ç–µ–≥–æ–≤

            –≤—ã–≤–æ–¥:
                ([—Ç–µ–≥–∏], –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)
            """
            if not self.api_key or self.api_key == "–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å":
                print("–û—à–∏–±–∫–∞: API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
                return [], 'neutral', 'API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'

            if not text or len(text.strip()) < 10:
                print("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return [], 'neutral', '–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π'

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏
            tags_list_str = "\n".join([f"- {tag}" for tag in self.PREDEFINED_TAGS])

            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –∏ –≤—ã–ø–æ–ª–Ω–∏ —Ç—Ä–∏ –∑–∞–¥–∞–Ω–∏—è:

    1. –¢–ï–ì–ò: –í—ã–±–µ—Ä–∏ —Ä–æ–≤–Ω–æ 3-5 —Å–∞–º—ã—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ–≥–æ–≤ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.
       - –í—ã–±–∏—Ä–∞–π —Ç–µ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ –æ–ø–∏—Å—ã–≤–∞—é—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
       - –¢–æ–ª—å–∫–æ —Ç–µ–≥–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–∏–∂–µ, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ
       - –¢–µ–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–∫—Å—Ç–∞

    2. –ù–ê–°–¢–†–û–ï–ù–ò–ï: –û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –æ—Ö–∞—Ä–∫—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏ –°–±–µ—Ä–ë–∞–Ω–∫
       - positive (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π, –æ–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω—ã–π)
       - neutral (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π) 
       - negative (–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π)

    3. –û–ë–™–Ø–°–ù–ï–ù–ò–ï: –ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏ –ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω—ã –∏–º–µ–Ω–Ω–æ —ç—Ç–∏ —Ç–µ–≥–∏ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
       - –û–±—ä—è—Å–Ω–∏ —Å–≤—è–∑—å –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏
       - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É

    –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ–≥–∏:
    {tags_list_str}

    –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
    {text[:1500]}

    –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π, —ç—Ç–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ!):
    –¢–ï–ì–ò: —Ç–µ–≥1, —Ç–µ–≥2, —Ç–µ–≥3, —Ç–µ–≥4, —Ç–µ–≥5
    –ù–ê–°–¢–†–û–ï–ù–ò–ï: positive|neutral|negative
    –û–ë–™–Ø–°–ù–ï–ù–ò–ï: –ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω—ã –∏–º–µ–Ω–Ω–æ —ç—Ç–∏ —Ç–µ–≥–∏

    –ü—Ä–∏–º–µ—Ä:
    –¢–ï–ì–ò: —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
    –ù–ê–°–¢–†–û–ï–ù–ò–ï: positive
    –û–ë–™–Ø–°–ù–ï–ù–ò–ï: –í —Ç–µ–∫—Å—Ç–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª—è—Ö, —á—Ç–æ –Ω–∞–ø—Ä—è–º—É—é —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º–∏."""

            try:
                response = requests.post(
                    self.api_url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {self.api_key}"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–æ–≤. 
                                –í–ê–ñ–ù–û: –°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π —Ñ–æ—Ä–º–∞—Ç—É –æ—Ç–≤–µ—Ç–∞.
                                –í—ã–±–∏—Ä–∞–π —Ç–µ–≥–∏ –¢–û–õ–¨–ö–û –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.
                                –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."""
                            },
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        "temperature": 0.3,
                        "max_tokens": 250,  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                        "stream": False
                    },
                    timeout=30
                )

                # –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ—Ç–≤–µ—Ç–∞
                if response.status_code != 200:
                    print(f" API –æ—à–∏–±–∫–∞: HTTP {response.status_code}")
                    print(f"   –û—Ç–≤–µ—Ç: {response.text[:200]}")
                    return [], 'neutral', f'API –æ—à–∏–±–∫–∞: {response.status_code}'

                # –ø–∞—Ä—Å–∏–º JSON
                result = response.json()

                if 'error' in result:
                    print(f" API –æ—à–∏–±–∫–∞: {result['error'].get('message', 'Unknown error')}")
                    return [], 'neutral', f"API –æ—à–∏–±–∫–∞: {result['error'].get('message', 'Unknown error')}"

                # –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç LLM
                if result.get('choices') and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content'].strip()
                    print(f"–û—Ç–≤–µ—Ç LLM:\n{content}\n{'-'*50}")

                    # –ø–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
                    tags = self._parse_tags(content)
                    sentiment = self._parse_sentiment(content)
                    explanation = self._parse_explanation(content)

                    return tags, sentiment, explanation
                else:
                    return [], 'neutral', '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM'

            except requests.exceptions.Timeout:
                print("–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API")
                return [], 'neutral', '–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API'
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {e}")
                return [], 'neutral', f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:100]}'

    def _parse_tags(self, content: str) -> List[str]:
        """–ø–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        tags_match = re.search(r'–¢–ï–ì–ò:\s*(.+?)(?:\n|$)', content, re.IGNORECASE)
        if tags_match:
            tags_text = tags_match.group(1).strip()
            tags = [tag.strip() for tag in tags_text.split(',')]
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–≥–∏ –∏–∑ —Å–ø–∏—Å–∫–∞
            valid_tags = []
            for tag in tags:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if tag in self.PREDEFINED_TAGS and tag not in valid_tags:
                    valid_tags.append(tag)
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞
                    for predefined_tag in self.PREDEFINED_TAGS:
                        if predefined_tag.lower() == tag.lower() and predefined_tag not in valid_tags:
                            valid_tags.append(predefined_tag)
                            break
            
            # –ï—Å–ª–∏ —Ç–µ–≥–æ–≤ –º–∞–ª–æ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
            if len(valid_tags) < 2 and len(tags) > 0:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–µ–≥–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                tag_lower = tags[0].lower()
                for predefined_tag in self.PREDEFINED_TAGS:
                    if tag_lower in predefined_tag.lower() and predefined_tag not in valid_tags:
                        valid_tags.append(predefined_tag)
                        if len(valid_tags) >= 3:
                            break
            
            return valid_tags[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 5 —Ç–µ–≥–æ–≤
        
        return []
    
    def _parse_sentiment(self, content: str) -> str:
        """–ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        sentiment = 'neutral'
        sentiment_match = re.search(r'–ù–ê–°–¢–†–û–ï–ù–ò–ï:\s*(positive|neutral|negative)', content, re.IGNORECASE)
        if sentiment_match:
            sentiment = sentiment_match.group(1).lower()
        
        return sentiment
    
    def _parse_explanation(self, content: str) -> str:
        """–ø–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        explanation = '–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ'
        
        # –ò—â–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –º–µ—Ç–∫–∏ –û–ë–™–Ø–°–ù–ï–ù–ò–ï:
        explanation_match = re.search(r'–û–ë–™–Ø–°–ù–ï–ù–ò–ï:\s*(.+?)(?:\n\n|\n$|$)', content, re.IGNORECASE | re.DOTALL)
        if explanation_match:
            explanation = explanation_match.group(1).strip()
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            if len(explanation) > 500:
                explanation = explanation[:497] + "..."
        else:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –±–µ–∑ –º–µ—Ç–∫–∏
            lines = content.split('\n')
            for i, line in enumerate(lines):
                if '–¢–ï–ì–ò:' in line and i + 3 < len(lines):
                    # –í–æ–∑–º–æ–∂–Ω–æ, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ 2-3 —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ —Ç–µ–≥–æ–≤
                    for j in range(1, 4):
                        if i + j < len(lines):
                            potential_explanation = lines[i + j].strip()
                            if (potential_explanation and 
                                not potential_explanation.startswith('–ù–ê–°–¢–†–û–ï–ù–ò–ï:') and
                                not potential_explanation.startswith('–¢–ï–ì–ò:') and
                                not potential_explanation.startswith('–£–í–ï–†–ï–ù–ù–û–°–¢–¨:')):
                                explanation = potential_explanation
                                break
        
        return explanation
    
    def save_to_db(self, chunk_id: int, llm_tags: List[str], 
                   sentiment: str, explanation: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î"""
        if not os.path.exists(self.db_path):
            print(f"–ë–î {self.db_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ")
            return
            
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE chunks 
                SET llm_tags = ?, 
                    sentiment = ?,
                    explanation = ?
                WHERE id = ?
            """, (
                json.dumps(llm_tags, ensure_ascii=False),
                sentiment,
                explanation,
                chunk_id
            ))
            
            conn.commit()
    
    def process_chunk(self, chunk: ChunkData, delay: float = 0.5) -> Dict:
        """
        –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        """
        print(f"\n{'='*60}")
        print(f"üìã –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ #{chunk.chunk_id}")
        print(f"{'='*60}")
        print(f"üìù –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"   {chunk.text[:200]}...")
        
        # –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        llm_tags, sentiment, explanation = self.analyze_with_llm(chunk.text)
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(llm_tags) if llm_tags else '–Ω–µ –≤—ã–±—Ä–∞–Ω—ã'}")
        print(f"   üòä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment}")
        print(f"   üìù –û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {explanation}")
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        self.save_to_db(
            chunk.chunk_id, llm_tags, sentiment, explanation)
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î")
        
        # –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è rate limit
        time.sleep(delay)
        
        return {
            'chunk_id': chunk.chunk_id,
            'llm_tags': llm_tags,
            'sentiment': sentiment,
            'explanation': explanation,
        }
    
    def process_all(self, limit: Optional[int] = None, delay: float = 0.5):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ –ë–î
        """
        chunks = self.get_chunks(limit=limit)
        
        if not chunks:
            print("‚ÑπÔ∏è  –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return []
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        results = []
        for i, chunk in enumerate(chunks, 1):
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {i}/{len(chunks)}")
            
            try:
                result = self.process_chunk(chunk, delay=delay)
                results.append(result)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ chunk {chunk.chunk_id}: {e}")
                continue
        
        print(f"\n{'='*60}")
        print(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)} —á–∞–Ω–∫–æ–≤")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if results:
            sentiments = [r['sentiment'] for r in results]
            from collections import Counter
            sentiment_stats = Counter(sentiments)
            
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π:")
            for sentiment, count in sentiment_stats.items():
                print(f"   {sentiment}: {count}")
        
        return results

    def analyze_text(self, text: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ë–î
        """
        print(f"\n{'='*60}")
        print(f"üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞")
        print(f"{'='*60}")
        print(f"üìù –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"   {text[:200]}...")
        
        llm_tags, sentiment, explanation = self.analyze_with_llm(text)
        
        result = {
            'llm_tags': llm_tags,
            'sentiment': sentiment,
            'explanation': explanation,
        }
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(llm_tags)}")
        print(f"   üòä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment}")
        print(f"   üìù –û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {explanation}")
        
        return result


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    """
    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    classifier = LLMMetadataClassifier(
        db_path="telegram_data.db",
        api_key=API_KEY
    )
    
    if API_KEY is None:
        print("‚ùå –û—à–∏–±–∫–∞: GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        print("üìù –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º: GROQ_API_KEY=–≤–∞—à_–∫–ª—é—á")
        return
    
    # –æ–±—Ä–∞–±–æ—Ç–∫–∞
    results = classifier.process_all(
        limit=5,
        delay=0.5 
    )
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if results:
        print(f"\nüìã –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        for result in results:
            print(f"\n–ß–∞–Ω–∫ {result['chunk_id']}:")
            print(f"  üè∑Ô∏è  {', '.join(result['llm_tags'])}")
            print(f"  üòä {result['sentiment']}")
            print(f"  üìù {result['explanation']}")


if __name__ == "__main__":
    main()