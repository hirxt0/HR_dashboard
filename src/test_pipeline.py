import os
from dotenv import load_dotenv
import json
import numpy as np
from run_pipeline import main

load_dotenv()


def create_test_data():
    """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    test_dir = "test_data"
    os.makedirs(test_dir, exist_ok=True)
    
    # 1. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ .txt —Ñ–∞–π–ª—ã
    test_texts = [
        "–†—ã–Ω–æ–∫ IT –≤ 2024 –≥–æ–¥—É –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–æ—Å—Ç. –ö—Ä—É–ø–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ AI.",
        "–ü—Ä–æ–±–ª–µ–º—ã —Å –ª–æ–≥–∏—Å—Ç–∏–∫–æ–π –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç –≤–ª–∏—è—Ç—å –Ω–∞ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä–æ–∑–Ω–∏—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–µ.",
        "–ù–æ–≤—ã–π –∑–∞–∫–æ–Ω –æ –Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω–∏–∏ –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –º–∞–ª—ã–π –±–∏–∑–Ω–µ—Å. –≠–∫—Å–ø–µ—Ä—Ç—ã –æ–∂–∏–¥–∞—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π.",
        "–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞—è–≤–ª–µ–Ω–∏–π —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤.",
        "–ó–µ–ª—ë–Ω–∞—è —ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ –ø–æ–ª—É—á–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞. –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ —Å–æ–ª–Ω–µ—á–Ω—ã–µ –ø–∞–Ω–µ–ª–∏ —Ä–∞—Å—Ç—É—Ç.",
        "–í IT –æ–≥—Ä–æ–º–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"
    ]
    
    for i, text in enumerate(test_texts):
        with open(os.path.join(test_dir, f"doc_{i}.txt"), "w", encoding="utf-8") as f:
            f.write(text)
    
    # 2. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π .csv —Ñ–∞–π–ª
    import pandas as pd
    csv_data = {
        "text": [
            "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á—ë—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ–∫–∞–∑–∞–ª –ø—Ä–∏–±—ã–ª—å –≤—ã—à–µ –æ–∂–∏–¥–∞–Ω–∏–π.",
            "–ê–∫—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞ —É–ø–∞–ª–∏ –Ω–∞ 2% —Å–µ–≥–æ–¥–Ω—è.",
            "–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫ —Å–æ—Ö—Ä–∞–Ω–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π."
        ],
        "source": ["news_1", "news_2", "news_3"]
    }
    df = pd.DataFrame(csv_data)
    df.to_csv(os.path.join(test_dir, "news.csv"), index=False, encoding="utf-8")
    
    return test_dir


def load_chunks_from_output(output_folder="test_output"):
    """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    chunks_file = os.path.join(output_folder, "chunks.jsonl")
    
    if not os.path.exists(chunks_file):
        raise FileNotFoundError(f"–§–∞–π–ª {chunks_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω —Å–Ω–∞—á–∞–ª–∞!")
    
    chunks = []
    with open(chunks_file, "r", encoding="utf-8") as f:
        for line in f:
            chunks.append(json.loads(line))
    
    return chunks


def generate_test_queries_from_data(chunks, n=5):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    import random
    
    test_queries = []
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —á–∞–Ω–∫–∏
    sample_chunks = random.sample(chunks, min(n, len(chunks)))
    
    for chunk in sample_chunks:
        text = chunk['text']
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
        sentences = text.split('.')
        if sentences:
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –¥–µ–ª–∞–µ–º –∏–∑ –Ω–µ–≥–æ –≤–æ–ø—Ä–æ—Å
            first_sentence = sentences[0].strip()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if "IT" in text or "AI" in text or "—Ç–µ—Ö–Ω–æ–ª–æ–≥" in text:
                query = "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ AI"
            elif "–ª–æ–≥–∏—Å—Ç–∏–∫" in text or "—Ç–æ—Ä–≥–æ–≤–ª" in text:
                query = "–ö–∞–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –∏ —Ç–æ—Ä–≥–æ–≤–ª–µ?"
            elif "–Ω–∞–ª–æ–≥" in text or "–∑–∞–∫–æ–Ω" in text:
                query = "–ß—Ç–æ –Ω–æ–≤–æ–≥–æ –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ –∏ –Ω–∞–ª–æ–≥–∞—Ö?"
            elif "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç" in text or "—Ä–µ–≥—É–ª—è—Ç–æ—Ä" in text:
                query = "–°–∏—Ç—É–∞—Ü–∏—è —Å –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞–º–∏?"
            elif "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫" in text or "–∏–Ω–≤–µ—Å—Ç–∏—Ü" in text:
                query = "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏–∫—É?"
            else:
                # –û–±—â–∏–π –≤–æ–ø—Ä–æ—Å
                query = f"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ {first_sentence[:50]}"
            
            test_queries.append({
                "query": query,
                "expected_chunk_id": chunk['chunk_id'],
                "expected_text": text[:100]
            })
    
    return test_queries


def test_rag_with_real_data(output_folder="test_output", config_path="test_config.yaml"):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º RAG —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    from embeddings import GetEmbeddings
    from rag import RAG
    from utils import load_config
    
    print("\n" + "="*60)
    print("RAG –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò")
    print("="*60)
    
    # 0. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—É –∂–µ –º–æ–¥–µ–ª—å
    cfg = load_config(config_path)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞
    print("\nüìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
    chunks = load_chunks_from_output(output_folder)
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG...")
    rag = RAG(cfg)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
    index_path = os.path.join(output_folder, "indices", "faiss.index")
    map_path = os.path.join(output_folder, "indices", "id_map.json")
    
    if os.path.exists(index_path):
        rag.load_index(index_path, map_path)
        rag.id_to_chunk = {i: chunks[i] for i in range(len(chunks))}
        print("‚úì FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω")
    else:
        print("‚úó –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π...")
        rag.build_index(chunks, os.path.join(output_folder, "indices"))
    
    # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    print("\nüé≤ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_queries = generate_test_queries_from_data(chunks, n=5)
    
    # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º embedder —Å —Ç–æ–π –∂–µ –º–æ–¥–µ–ª—å—é —á—Ç–æ –∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    print(f"\nü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {cfg['embeddings']['model_name']}")
    emb_model = GetEmbeddings(
        chunk_size=cfg["embeddings"]["chunk_size"],
        chunk_overlap=cfg["embeddings"]["chunk_overlap"],
        model_name=cfg["embeddings"]["model_name"]
    )
    
    # 5. –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
    print("\n" + "="*60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*60)
    
    total_tests = len(test_queries)
    passed_tests = 0
    
    for i, test in enumerate(test_queries, 1):
        query = test['query']
        expected_chunk_id = test['expected_chunk_id']
        
        print(f"\n{'‚îÄ'*60}")
        print(f"–¢–ï–°–¢ {i}/{total_tests}")
        print(f"{'‚îÄ'*60}")
        print(f"üìù –ó–∞–ø—Ä–æ—Å: {query}")
        print(f"üéØ –û–∂–∏–¥–∞–µ–º—ã–π —á–∞–Ω–∫: {expected_chunk_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_emb = emb_model.embedding([query])[0]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = rag.query(query_emb, top_k=3)
        
        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        found_expected = False
        for rank, r in enumerate(results, 1):
            chunk_id = r['chunk']['chunk_id']
            score = r['score']
            text_preview = r['chunk']['text'][:80]
            
            marker = "‚úì" if chunk_id == expected_chunk_id else " "
            print(f"\n{marker} –†–µ–∑—É–ª—å—Ç–∞—Ç #{rank}:")
            print(f"   ID: {chunk_id}")
            print(f"   Score: {score:.4f}")
            print(f"   –¢–µ–∫—Å—Ç: {text_preview}...")
            
            if chunk_id == expected_chunk_id:
                found_expected = True
                print(f"   üéâ –ù–ê–ô–î–ï–ù –æ–∂–∏–¥–∞–µ–º—ã–π —á–∞–Ω–∫ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {rank}!")
        
        if found_expected:
            passed_tests += 1
            print(f"\n‚úÖ –¢–µ—Å—Ç {i} –ü–†–û–ô–î–ï–ù")
        else:
            print(f"\n‚ùå –¢–µ—Å—Ç {i} –ù–ï –ü–†–û–ô–î–ï–ù (–æ–∂–∏–¥–∞–µ–º—ã–π —á–∞–Ω–∫ –Ω–µ –≤ —Ç–æ–ø-3)")
    
    # 6. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*60)
    print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*60)
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
    print(f"–ü—Ä–æ–π–¥–µ–Ω–æ: {passed_tests}")
    print(f"–ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {total_tests - passed_tests}")
    print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {(passed_tests/total_tests)*100:.1f}%")
    
    if passed_tests == total_tests:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
    elif passed_tests >= total_tests * 0.6:
        print("\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–µ–º–ª–µ–º—ã–π –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞")
    else:
        print("\n‚ö† –¢—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã")


def interactive_rag_test(output_folder="test_output", config_path="test_config.yaml"):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG"""
    from embeddings import GetEmbeddings
    from rag import RAG
    from utils import load_config
    
    print("\n" + "="*60)
    print("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï RAG")
    print("="*60)
    print("–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–∏ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RAG —Å–∏—Å—Ç–µ–º—ã")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit' –∏–ª–∏ 'quit'")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
    cfg = load_config(config_path)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    chunks = load_chunks_from_output(output_folder)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG
    rag = RAG(cfg)
    index_path = os.path.join(output_folder, "indices", "faiss.index")
    map_path = os.path.join(output_folder, "indices", "id_map.json")
    
    rag.load_index(index_path, map_path)
    rag.id_to_chunk = {i: chunks[i] for i in range(len(chunks))}
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å —á—Ç–æ –∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    emb_model = GetEmbeddings(
        chunk_size=cfg["embeddings"]["chunk_size"],
        chunk_overlap=cfg["embeddings"]["chunk_overlap"],
        model_name=cfg["embeddings"]["model_name"]
    )
    
    while True:
        query = input("\nüîç –í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()
        
        if query.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if not query:
            continue
        
        # –ü–æ–∏—Å–∫
        query_emb = emb_model.embedding([query])[0]
        results = rag.query(query_emb, top_k=5)
        
        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\n")
        
        for i, r in enumerate(results, 1):
            print(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç #{i} (score: {r['score']:.4f})")
            print(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print(f"ID: {r['chunk']['chunk_id']}")
            print(f"–¢–µ–∫—Å—Ç: {r['chunk']['text']}")
            print()


def main_test():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("="*60)
    print("–ù–ê–ß–ê–õ–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ü–ê–ô–ü–õ–ê–ô–ù–ê")
    print("="*60)
    
    # 1. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("\n1Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_dir = create_test_data()
    print(f"‚úì –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã –≤ {test_dir}")
    
    # 2. –û–±–Ω–æ–≤–ª—è–µ–º config –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    config_content = f"""data:
  input_folder: "{test_dir}"
  rss_feeds: []

embeddings:
  model_name: "sentence-transformers/all-mpnet-base-v2"  # ‚Üê –¢–ê–ö–ê–Ø –ñ–ï –ö–ê–ö –í –û–°–ù–û–í–ù–û–ú CONFIG!
  chunk_size: 400
  chunk_overlap: 100
  batch_size: 8

clustering:
  algorithm: "dbscan"
  hdbscan_min_cluster_size: 2
  dbscan_eps: 0.3
  dbscan_min_samples: 2

llm:
  mode: "mock"
  provider: "gigachat"

rag:
  top_k: 3
  min_score: 0.0

output:
  out_folder: "test_output"
"""
    
    with open("test_config.yaml", "w", encoding="utf-8") as f:
        f.write(config_content)
    print("‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω–∞")
    
    # 3. –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    print("\n2Ô∏è‚É£ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
    try:
        main("test_config.yaml")
        print("‚úì –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ!")
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n3Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
    output_files = [
        "test_output/chunks.jsonl",
        "test_output/clusters.json",
        "test_output/chunks_clusters.json",
        "test_output/indices/faiss.index"
    ]
    
    for file in output_files:
        if os.path.exists(file):
            print(f"‚úì {file}")
            if file.endswith(".json"):
                try:
                    with open(file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    print(f"  ‚îî‚îÄ –ó–∞–ø–∏—Å–µ–π: {len(data) if isinstance(data, list) else 'dict'}")
                except:
                    pass
        else:
            print(f"‚úó {file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # 5. –¢–µ—Å—Ç–∏—Ä—É–µ–º RAG —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    print("\n4Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...")
    try:
        test_rag_with_real_data("test_output", "test_config.yaml")  # ‚Üê –ü–µ—Ä–µ–¥–∞—ë–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –≤ RAG —Ç–µ—Å—Ç–µ: {e}")
        import traceback
        traceback.print_exc()
    
    # 6. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
    clusters_file = "test_output/clusters.json"
    if os.path.exists(clusters_file):
        print("\n5Ô∏è‚É£ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
        with open(clusters_file, "r", encoding="utf-8") as f:
            clusters = json.load(f)
        
        for cluster_id, info in clusters.items():
            if cluster_id != "-1":
                print(f"\nüî∏ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}:")
                print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {info.get('name_short', '–ù/–î')}")
                print(f"   –†–∞–∑–º–µ—Ä: {info.get('size', 0)}")
                print(f"   –¢–µ–≥–∏: {', '.join(info.get('top_tags', []))}")
    
    # 7. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\n" + "="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*60)
    
    answer = input("\nüí° –•–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å RAG –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ? (y/n): ")
    if answer.lower() in ['y', 'yes', '–¥', '–¥–∞']:
        interactive_rag_test("test_output", "test_config.yaml")


if __name__ == "__main__":
    main_test()