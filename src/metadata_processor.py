# –ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ –Ω–∞–±—Ä–æ—Å–æ–∫ 


from transformers import pipeline
import re
from typing import List, Dict
from collections import Counter
import numpy as np

# –î–ª—è KeyBERT
try:
    from keybert import KeyBERT
    KEYBERT_AVAILABLE = True
except ImportError:
    KEYBERT_AVAILABLE = False
    print("‚ö†Ô∏è KeyBERT –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install keybert")

# –î–ª—è YAKE
try:
    import yake
    YAKE_AVAILABLE = True
except ImportError:
    YAKE_AVAILABLE = False
    print("‚ö†Ô∏è YAKE –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install yake")


class MetadataProcessorRU:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–µ–π —Ç–µ–≥–æ–≤
    """
    
    def __init__(self, tag_extraction_method: str = 'keybert'):
        """
        Args:
            tag_extraction_method: 'keybert', 'yake', –∏–ª–∏ 'frequency'
        """
        print(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MetadataProcessorRU...")
        print(f"   –ú–µ—Ç–æ–¥ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏ —Ç–µ–≥–æ–≤: {tag_extraction_method}")
        
        self.tag_method = tag_extraction_method
        
        # Sentiment –º–æ–¥–µ–ª—å
        try:
            self.sentiment_model = pipeline(
                'sentiment-analysis',
                model='blanchefort/rubert-base-cased-sentiment',
                device=-1  # CPU
            )
            print("   ‚úì Sentiment: blanchefort/rubert-base-cased-sentiment")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Sentiment –æ—à–∏–±–∫–∞: {e}")
            self.sentiment_model = None
        
        # KeyBERT –º–æ–¥–µ–ª—å
        self.keybert_model = None
        if tag_extraction_method == 'keybert' and KEYBERT_AVAILABLE:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
                self.keybert_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')
                print("   ‚úì KeyBERT: paraphrase-multilingual-MiniLM-L12-v2")
            except Exception as e:
                print(f"   ‚ö†Ô∏è KeyBERT –æ—à–∏–±–∫–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                self.tag_method = 'yake'
        
        # YAKE —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
        self.yake_extractor = None
        if tag_extraction_method == 'yake' and YAKE_AVAILABLE:
            try:
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
                self.yake_extractor = yake.KeywordExtractor(
                    lan="ru",
                    n=2,  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ n-–≥—Ä–∞–º–º
                    dedupLim=0.7,
                    dedupFunc='seqm',
                    windowsSize=1,
                    top=10
                )
                print("   ‚úì YAKE: —Ä—É—Å—Å–∫–∏–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä")
            except Exception as e:
                print(f"   ‚ö†Ô∏è YAKE –æ—à–∏–±–∫–∞: {e}")
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        self.stopwords = {
            '–∏','–≤','–≤–æ','–Ω–µ','—á—Ç–æ','–æ–Ω','–Ω–∞','—è','—Å','—Å–æ','–∫–∞–∫','–∞','—Ç–æ','–≤—Å–µ',
            '–æ–Ω–∞','—Ç–∞–∫','–µ–≥–æ','–Ω–æ','–¥–∞','—Ç—ã','–∫','—É','–∑–∞','–æ—Ç','–∏–∑','–ø–æ','–¥–ª—è',
            '–æ','–æ–±','–∂–µ','–∏–ª–∏','–µ—Å–ª–∏','–∫–æ–≥–¥–∞','–±—ã','–µ–µ','–æ–Ω–∏','–º—ã','–º–æ–π','—Ç–≤–æ–π',
            '–µ–µ','–∏—Ö','–±—ã—Ç—å','—ç—Ç–æ','—Ç–∞–∫–∂–µ','–≤—Å—ë','—Ç–æ–≥–æ','–µ—Å—Ç—å','–±—ã–ª','–±—ã–ª–∞','–±—ã–ª–∏',
            '–±—É–¥–µ—Ç','–º–æ–∂–µ—Ç','–æ—á–µ–Ω—å','—É–∂–µ','—Ç–æ–ª—å–∫–æ','–±–æ–ª–µ–µ','–º–æ–∂–Ω–æ','—Ç–∞–∫–æ–π','—Ç–∞–∫–∞—è','–≥–æ–¥',
            '–≥–æ–≤–æ—Ä–∏—Ç','—Å–∫–∞–∑–∞–ª','—Å—Ç–∞–ª','—Å—Ç–∞–ª–∞','—Å—Ç–∞–ª–∏','–±—É–¥—É—Ç','–±—ã–ª–∏'
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏–Ω—Å–∞–π–¥–æ–≤
        self.insider_patterns = [
            r'—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ',
            r'–∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–æ–æ–±—â–∞—é—Ç',
            r'–∫–∞–∫ —Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ',
            r'–∏–Ω—Å–∞–π–¥–µ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç',
            r'–ø–æ –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
            r'–æ—Ç –∞–Ω–æ–Ω–∏–º–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞',
            r'–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ',
            r'—É—Ç–µ—á–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
            r'—ç–∫—Å–∫–ª—é–∑–∏–≤',
            r'–ø–æ –¥–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤',
            r'–∫–∞–∫ —É–∑–Ω–∞–ª(–∞|–∏)?',
            r'—Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤',
            r'–Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ',
            r'–∏–Ω—Å–∞–π–¥'
        ]
        
        print("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n")
    
    def extract_tags(self, text: str, top_n: int = 5) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        """
        if not text or len(text) < 20:
            return []
        
        try:
            # KeyBERT - –Ω–∞–∏–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            if self.tag_method == 'keybert' and self.keybert_model:
                return self._extract_tags_keybert(text, top_n)
            
            # YAKE - –±—ã—Å—Ç—Ä—ã–π –∏ —Ö–æ—Ä–æ—à–∏–π
            elif self.tag_method == 'yake' and self.yake_extractor:
                return self._extract_tags_yake(text, top_n)
            
            # Fallback - —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            else:
                return self._extract_tags_frequency(text, top_n)
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏ —Ç–µ–≥–æ–≤: {e}")
            return self._extract_tags_frequency(text, top_n)
    
    def _extract_tags_keybert(self, text: str, top_n: int) -> List[str]:
        """–≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —á–µ—Ä–µ–∑ KeyBERT"""
        keywords = self.keybert_model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 2),  # 1-2 —Å–ª–æ–≤–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            stop_words=list(self.stopwords),
            top_n=top_n * 2,  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            diversity=0.7  # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        )
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
        tags = []
        for keyword, score in keywords:
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ
            keyword = keyword.lower().strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —á–∏—Å–ª–∞
            if len(keyword) < 3 or keyword.isdigit():
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            words = keyword.split()
            if all(w in self.stopwords for w in words):
                continue
            
            tags.append(keyword)
            
            if len(tags) >= top_n:
                break
        
        return tags
    
    def _extract_tags_yake(self, text: str, top_n: int) -> List[str]:
        """–≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —á–µ—Ä–µ–∑ YAKE"""
        keywords = self.yake_extractor.extract_keywords(text)
        
        tags = []
        for keyword, score in keywords:
            keyword = keyword.lower().strip()
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if len(keyword) < 3:
                continue
            
            words = keyword.split()
            if all(w in self.stopwords for w in words):
                continue
            
            tags.append(keyword)
            
            if len(tags) >= top_n:
                break
        
        return tags
    
    def _extract_tags_frequency(self, text: str, top_n: int) -> List[str]:
        """Fallback: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        text_norm = re.sub(r"[^–∞-—è—ëa-z0-9\s]", " ", text.lower())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã —Ç–æ–∂–µ
        words = text_norm.split()
        
        # –£–Ω–∏–≥—Ä–∞–º–º—ã
        unigrams = [w for w in words if len(w) >= 4 and w not in self.stopwords]
        
        # –ë–∏–≥—Ä–∞–º–º—ã
        bigrams = []
        for i in range(len(words) - 1):
            if words[i] not in self.stopwords or words[i+1] not in self.stopwords:
                bigram = f"{words[i]} {words[i+1]}"
                if len(bigram) >= 6:
                    bigrams.append(bigram)
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
        all_grams = unigrams + bigrams
        counts = Counter(all_grams)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞—Å—Ç—ã–µ, –Ω–æ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ
        meaningful = []
        for gram, count in counts.most_common(top_n * 2):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ 1 —Ä–∞–∑
            if count < 2 and len(all_grams) > 10:
                continue
            
            meaningful.append(gram)
            
            if len(meaningful) >= top_n:
                break
        
        return meaningful
    
    def analyze_sentiment(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.sentiment_model:
            return {'sentiment': 'neutral', 'score': 0.0}
        
        try:
            truncated = text[:512]
            result = self.sentiment_model(truncated)[0]
            
            label = result['label'].lower()
            sentiment_map = {
                'positive': 'positive',
                'neutral': 'neutral',
                'negative': 'negative'
            }
            
            return {
                'sentiment': sentiment_map.get(label, 'neutral'),
                'score': float(result['score'])
            }
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ sentiment: {e}")
            return {'sentiment': 'neutral', 'score': 0.0}
    
    def detect_insider(self, text: str) -> Dict:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∏–Ω—Å–∞–π–¥–µ—Ä—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        text_lower = text.lower()
        matches = 0
        matched_patterns = []
        
        for pattern in self.insider_patterns:
            if re.search(pattern, text_lower):
                matches += 1
                matched_patterns.append(pattern)
        
        is_insider = matches >= 2
        confidence = min(matches * 0.25, 1.0)
        
        return {
            'is_insider': is_insider,
            'confidence': float(confidence),
            'matched_patterns': matches
        }
    
    def classify_topic(self, text: str, tags: List[str] = None) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–º—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"""
        if tags is None:
            tags = []
        
        text_lower = text.lower()
        tags_lower = ' '.join(tags).lower()
        combined = text_lower + ' ' + tags_lower
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Å–∞–º–∏
        topic_keywords = {
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': {
                'keywords': [
                    ('–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', 3), ('–Ω–µ–π—Ä–æ—Å–µ—Ç–∏', 3), ('–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 3),
                    ('–ò–ò', 2), ('ai', 2), ('ml', 2), ('deep learning', 3),
                    ('–æ–±–ª–∞—á–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 2), ('–æ–±–ª–∞–∫–æ', 1), ('cloud', 2),
                    ('—á–∏–ø—ã', 2), ('–ø–æ–ª—É–ø—Ä–æ–≤–æ–¥–Ω–∏–∫–∏', 2), ('gpu', 2), ('nvidia', 2),
                    ('–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', 2), ('—Ä–æ–±–æ—Ç—ã', 2), ('—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞', 2),
                    ('–∫–≤–∞–Ω—Ç–æ–≤—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã', 3), ('–±–ª–æ–∫—á–µ–π–Ω', 2), ('–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è', 2),
                    ('–±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ', 2), ('big data', 2), ('–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö', 2),
                    ('–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 2), ('—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 1), ('—Å–æ—Ñ—Ç', 1),
                    ('—Å—Ç–∞—Ä—Ç–∞–ø', 1), ('—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π', 1), ('—Ü–∏—Ñ—Ä–æ–≤', 1)
                ]
            },
            '–±–∏–∑–Ω–µ—Å': {
                'keywords': [
                    ('–∫–æ–º–ø–∞–Ω–∏', 1), ('–±–∏–∑–Ω–µ—Å', 2), ('—Ä—ã–Ω–æ–∫', 2), ('–≤—ã—Ä—É—á–∫', 2),
                    ('–ø—Ä–∏–±—ã–ª', 2), ('–∏–Ω–≤–µ—Å—Ç–∏—Ü', 2), ('–∞–∫—Ü–∏', 2), ('—Å–¥–µ–ª–∫', 2),
                    ('–∫–∞–ø–∏—Ç–∞–ª', 2), ('—Ñ–æ–Ω–¥', 2), ('—Å—Ç–∞—Ä—Ç–∞–ø', 2), ('–≤–µ–Ω—á—É—Ä', 2),
                    ('unicorn', 3), ('–µ–¥–∏–Ω–æ—Ä–æ–≥', 3), ('ipo', 2), ('—Å–ª–∏—è–Ω–∏', 2),
                    ('–ø–æ–≥–ª–æ—â–µ–Ω–∏', 2), ('–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤', 2), ('–∫–æ—Ä–ø–æ—Ä–∞—Ü', 2)
                ]
            },
            '–Ω–∞—É–∫–∞': {
                'keywords': [
                    ('–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏', 2), ('—É—á—ë–Ω', 2), ('–Ω–∞—É—á–Ω', 2), ('–æ—Ç–∫—Ä—ã—Ç–∏', 3),
                    ('—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', 2), ('–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏', 2), ('—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 2),
                    ('–ø—É–±–ª–∏–∫–∞—Ü–∏', 2), ('—Å—Ç–∞—Ç—å—è', 1), ('peer-review', 2),
                    ('–¥–æ–∫—Ç–æ—Ä–∞–Ω—Ç', 2), ('–¥–∏—Å—Å–µ—Ä—Ç–∞—Ü–∏', 2), ('–∞–∫–∞–¥–µ–º–∏–∏', 2)
                ]
            },
            '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': {
                'keywords': [
                    ('–∫—É—Ä—Å', 2), ('–æ–±—É—á–µ–Ω–∏', 2), ('–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏', 2), ('—Å—Ç—É–¥–µ–Ω—Ç', 2),
                    ('–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å', 2), ('–ª–µ–∫—Ü–∏', 2), ('—Å–µ–º–∏–Ω–∞—Ä', 2), ('–≤–µ–±–∏–Ω–∞—Ä', 2),
                    ('–æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å', 2), ('—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç', 2), ('—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 1),
                    ('—à–∫–æ–ª–∞', 1), ('–±—É—Ç–∫–µ–º–ø', 2), ('bootcamp', 2)
                ]
            },
            '–ø–æ–ª–∏—Ç–∏–∫–∞': {
                'keywords': [
                    ('–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', 2), ('–∑–∞–∫–æ–Ω', 2), ('–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç', 2),
                    ('–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', 2), ('–¥—É–º–∞', 2), ('–º–∏–Ω–∏—Å—Ç—Ä', 2), ('–¥–µ–ø—É—Ç–∞—Ç', 2),
                    ('–≤–ª–∞—Å—Ç', 2), ('–ø–∞—Ä—Ç–∏—è', 2), ('–≤—ã–±–æ—Ä', 2), ('—Å–∞–Ω–∫—Ü–∏', 2),
                    ('—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', 2), ('–≥–æ—Å—É—Å–ª—É–≥–∏', 2)
                ]
            },
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞': {
                'keywords': [
                    ('—ç–∫–æ–Ω–æ–º–∏–∫', 2), ('–∏–Ω—Ñ–ª—è—Ü–∏', 2), ('—Ü–µ–Ω', 1), ('—Ç–∞—Ä–∏—Ñ', 2),
                    ('–∫—É—Ä—Å', 1), ('–¥–æ–ª–ª–∞—Ä', 2), ('—Ä—É–±–ª', 2), ('–±–∞–Ω–∫', 2),
                    ('–∫—Ä–µ–¥–∏—Ç', 2), ('–≤–≤–ø', 2), ('–±—é–¥–∂–µ—Ç', 2), ('–Ω–∞–ª–æ–≥', 2),
                    ('—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', 2), ('—Ü–±', 2), ('–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', 3)
                ]
            },
            '–º–µ–¥–∏–∞': {
                'keywords': [
                    ('–∫–æ–Ω—Ç–µ–Ω—Ç', 2), ('–±–ª–æ–≥–µ—Ä', 2), ('—Å—Ç—Ä–∏–º', 2), ('youtube', 2),
                    ('–ø–æ–¥–∫–∞—Å—Ç', 2), ('—Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏', 2), ('–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä', 2),
                    ('–º–µ–¥–∏–∞', 2), ('—Å–º–∏', 2), ('–Ω–æ–≤–æ—Å—Ç', 1), ('—Ä–µ–ø–æ—Ä—Ç–∞–∂', 2)
                ]
            },
            '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã': {
                'keywords': [
                    ('bitcoin', 3), ('–±–∏—Ç–∫–æ–∏–Ω', 3), ('ethereum', 3), ('—ç—Ñ–∏—Ä', 2),
                    ('–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç', 2), ('–±–ª–æ–∫—á–µ–π–Ω', 2), ('nft', 2), ('—Ç–æ–∫–µ–Ω', 2),
                    ('–º–∞–π–Ω–∏–Ω–≥', 2), ('defi', 2), ('web3', 2), ('–∫—Ä–∏–ø—Ç–æ', 2)
                ]
            }
        }
        
        # –ü–æ–¥—Å—á–µ—Ç —Å –≤–µ—Å–∞–º–∏
        topic_scores = {}
        for topic, config in topic_keywords.items():
            score = 0
            matches = []
            
            for keyword, weight in config['keywords']:
                count = combined.count(keyword)
                if count > 0:
                    weighted_score = count * weight
                    score += weighted_score
                    matches.append({
                        'keyword': keyword,
                        'count': count,
                        'weight': weight,
                        'score': weighted_score
                    })
            
            topic_scores[topic] = {
                'score': score,
                'matches': matches,
                'keyword_count': len(matches)
            }
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø —Ç–µ–º—ã
        scored_topics = {k: v['score'] for k, v in topic_scores.items() if v['score'] > 0}
        
        if scored_topics:
            main_topic = max(scored_topics, key=scored_topics.get)
        else:
            main_topic = '–æ–±—â–µ–µ'
        
        return {
            'main_topic': main_topic,
            'scores': {k: v['score'] for k, v in topic_scores.items()},
            'details': {k: v for k, v in topic_scores.items() if v['score'] > 0}
        }
    
    def process_chunk(self, chunk_id: str, text: str) -> Dict:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞"""
        tags = self.extract_tags(text, top_n=7)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 7
        sentiment_data = self.analyze_sentiment(text)
        topic_analysis = self.classify_topic(text, tags)
        insider_data = self.detect_insider(text)
        
        return {
            'chunk_id': chunk_id,
            'metadata': {
                'tags': tags,
                'sentiment': sentiment_data['sentiment'],
                'sentiment_score': sentiment_data['score'],
                'topic': topic_analysis['main_topic'],
                'topic_scores': topic_analysis['scores'],
                'topic_details': topic_analysis['details'],
                'is_insider': insider_data['is_insider'],
                'insider_confidence': insider_data['confidence']
            }
        }


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    test_text = """
    –ö–æ–º–ø–∞–Ω–∏—è OpenAI –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é GPT-5, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç 
    –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞. 
    –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ç–µ–ø–µ—Ä—å —Å–ø–æ—Å–æ–±–µ–Ω —Ä–µ—à–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ 
    –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.
    –ò–Ω–≤–µ—Å—Ç–æ—Ä—ã —É–∂–µ –ø—Ä–æ—è–≤–∏–ª–∏ –∏–Ω—Ç–µ—Ä–µ—Å –∫ –Ω–æ–≤–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ.
    """
    
    print("="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –≠–ö–°–¢–†–ê–ö–¶–ò–ò –¢–ï–ì–û–í")
    print("="*60)
    
    for method in ['keybert', 'yake', 'frequency']:
        print(f"\nüîπ –ú–µ—Ç–æ–¥: {method.upper()}")
        try:
            processor = MetadataProcessorRU(tag_extraction_method=method)
            tags = processor.extract_tags(test_text, top_n=7)
            print(f"   –¢–µ–≥–∏: {', '.join(tags)}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
