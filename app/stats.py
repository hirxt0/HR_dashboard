import json
from datetime import datetime
from collections import Counter
from typing import Dict, List, Any
from database import get_db_connection
from trend_analyzer import get_trend_signals  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å
import os

def get_dashboard_stats() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
    conn = get_db_connection()
    
    try:
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor = conn.execute("SELECT COUNT(*) as total FROM chunks")
        total_news = cursor.fetchone()['total']
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–≥–∏
        cursor.execute("SELECT llm_tags FROM chunks WHERE llm_tags IS NOT NULL AND llm_tags != ''")
        all_tags = []
        tag_errors = 0
        
        for row in cursor.fetchall():
            tags_json = row['llm_tags']
            if not tags_json or tags_json.strip() == '':
                continue
                
            try:
                tags = json.loads(tags_json)
                if isinstance(tags, list):
                    all_tags.extend(tags)
                else:
                    tag_errors += 1
            except json.JSONDecodeError:
                tag_errors += 1
            except Exception:
                tag_errors += 1
        
        unique_tags = len(set(all_tags))
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏ (—Ç–æ–ø-15)
        tag_counter = Counter(all_tags)
        popular_tags = [{'tag': tag, 'count': count} 
                       for tag, count in tag_counter.most_common(15)]
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç—Ä–µ–Ω–¥–æ–≤
        active_signals = get_trend_signals()
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        formatted_signals = []
        signal_id = 1
        
        for signal in active_signals:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è CSS –∫–ª–∞—Å—Å–æ–≤
            signal_type_map = {
                'problem': 'problem',
                'growing_problem': 'problem',
                'new_problem': 'problem',
                'opportunity': 'opportunity',
                'growing_opportunity': 'opportunity',
                'new_opportunity': 'opportunity',
                'new_trend': 'early_trend',
                'emerging_trend': 'early_trend'
            }
            
            css_type = signal_type_map.get(signal['type'], 'early_trend')
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å —Ç–µ–≥–æ–º
            tag = signal.get('tag', '–±–µ–∑ —Ç–µ–≥–∞')
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–∏–≥–Ω–∞–ª–∞
            sentiment = signal.get('sentiment_distribution', {})
            positive = sentiment.get('positive', 0)
            negative = sentiment.get('negative', 0)
            mentions = signal.get('mentions', 0)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            descriptions = {
                'problem': f"–¢–µ–≥ '{tag}' –∏–º–µ–µ—Ç {negative}% –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({mentions} –≤—Å–µ–≥–æ).",
                'growing_problem': f"‚ö†Ô∏è –ù–∞—Ä–∞—Å—Ç–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞! –¢–µ–≥ '{tag}': {negative}% –Ω–µ–≥–∞—Ç–∏–≤–∞, —Ä–æ—Å—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π.",
                'new_problem': f"üö® –ù–æ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞: —Ç–µ–≥ '{tag}' –ø–æ—è–≤–∏–ª—Å—è –Ω–µ–¥–∞–≤–Ω–æ, –Ω–æ —É–∂–µ {negative}% –Ω–µ–≥–∞—Ç–∏–≤–∞.",
                'opportunity': f"–¢–µ–≥ '{tag}' –∏–º–µ–µ—Ç {positive}% –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({mentions} –≤—Å–µ–≥–æ).",
                'growing_opportunity': f"üìà –†–∞—Å—Ç—É—â–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å! –¢–µ–≥ '{tag}': {positive}% –ø–æ–∑–∏—Ç–∏–≤–∞, —Ä–æ—Å—Ç –∏–Ω—Ç–µ—Ä–µ—Å–∞.",
                'new_opportunity': f"‚≠ê –ù–æ–≤–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å: —Ç–µ–≥ '{tag}' –±—ã—Å—Ç—Ä–æ –Ω–∞–±–∏—Ä–∞–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å {positive}% –ø–æ–∑–∏—Ç–∏–≤–∞.",
                'new_trend': f"üå± –ù–æ–≤—ã–π —Ç—Ä–µ–Ω–¥: —Ç–µ–≥ '{tag}' –ø–æ—è–≤–∏–ª—Å—è –Ω–µ–¥–∞–≤–Ω–æ ({mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π).",
                'emerging_trend': f"üöÄ –ó–∞—Ä–æ–∂–¥–∞—é—â–∏–π—Å—è —Ç—Ä–µ–Ω–¥: —Ç–µ–≥ '{tag}' –±—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—ë—Ç ({mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)."
            }
            
            description = descriptions.get(signal['type'], 
                f"–¢–µ–≥ '{tag}': {mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π, {positive}% –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö, {negative}% –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö.")
            
            formatted_signals.append({
                'id': signal_id,
                'title': signal.get('title', f'–°–∏–≥–Ω–∞–ª: {tag}'),
                'description': description,
                'type': css_type,
                'icon': signal.get('icon', 'fas fa-chart-line'),
                'mentions': mentions,
                'trend': signal.get('trend', 'stable'),
                'tag': tag,  # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–≥
                'sentiment': sentiment,
                'recommendations': signal.get('recommendations', []),
                'priority': signal.get('priority', 50),
                'first_seen': signal.get('first_seen', '?'),
                'last_seen': signal.get('last_seen', '?'),
                'details': {  # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    'signal_type': signal.get('type', 'unknown'),
                    'total_mentions': mentions,
                    'sentiment_breakdown': sentiment,
                    'trend_direction': signal.get('trend', 'stable'),
                    'days_active': signal.get('days_active', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                }
            })
            signal_id += 1
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ
        if not formatted_signals:
            formatted_signals = get_basic_signals(conn)
        
        return {
            'total_news': total_news,
            'unique_tags': unique_tags,
            'total_signals': len(active_signals),
            'popular_tags': popular_tags,
            'signals': formatted_signals[:6],  # –ú–∞–∫—Å–∏–º—É–º 6 —Å–∏–≥–Ω–∞–ª–æ–≤
            'update_time': datetime.now().strftime('%d.%m.%Y %H:%M'),
            'debug_info': {
                'total_tags_collected': len(all_tags),
                'tag_parsing_errors': tag_errors,
                'tag_samples': list(set(all_tags))[:10] if all_tags else [],
                'active_signals_count': len(active_signals)
            }
        }
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        import traceback
        traceback.print_exc()
        return {
            'total_news': 0,
            'unique_tags': 0,
            'total_signals': 0,
            'popular_tags': [],
            'signals': [],
            'update_time': datetime.now().strftime('%d.%m.%Y %H:%M'),
            'error': str(e)
        }
    finally:
        conn.close()

def get_basic_signals(conn) -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞—à–µ–ª —Ç—Ä–µ–Ω–¥–æ–≤"""
    print("\nüîç –ó–∞–ø—É—Å–∫ get_basic_signals()...")
    
    # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
    cursor = conn.execute('''
    SELECT id, text, llm_tags, sentiment, created_at
    FROM chunks 
    WHERE llm_tags IS NOT NULL AND llm_tags != '' 
    ORDER BY created_at DESC
    ''')
    
    all_rows = cursor.fetchall()
    print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —Å —Ç–µ–≥–∞–º–∏: {len(all_rows)}")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–≥–∞–º
    tag_stats = {}
    
    for row in all_rows:
        tags_json = row['llm_tags']
        sentiment = row['sentiment']
        created_at = row['created_at']
        
        if not tags_json:
            continue
            
        try:
            tags = json.loads(tags_json)
            if not isinstance(tags, list):
                continue
                
            for tag in tags:
                tag = tag.strip()
                if not tag:
                    continue
                    
                if tag not in tag_stats:
                    tag_stats[tag] = {
                        'total': 0, 
                        'positive': 0, 
                        'negative': 0, 
                        'neutral': 0,
                        'dates': []  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–∞—Ç
                    }
                
                tag_stats[tag]['total'] += 1
                if sentiment == 'positive':
                    tag_stats[tag]['positive'] += 1
                elif sentiment == 'negative':
                    tag_stats[tag]['negative'] += 1
                elif sentiment == 'neutral':
                    tag_stats[tag]['neutral'] += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç—É
                if created_at:
                    tag_stats[tag]['dates'].append(created_at)
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–≥–æ–≤: {e}")
            continue
    
    print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(tag_stats)}")
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤
    formatted_signals = []
    signal_id = 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–≥–∏ –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    sorted_tags = sorted(tag_stats.items(), key=lambda x: x[1]['total'], reverse=True)
    
    for tag, stats in sorted_tags[:15]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-15 –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º
        total = stats['total']
        
        if total < 3:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö
            continue
            
        positive_pct = (stats['positive'] / total * 100) if total > 0 else 0
        negative_pct = (stats['negative'] / total * 100) if total > 0 else 0
        neutral_pct = (stats['neutral'] / total * 100) if total > 0 else 0
        
        print(f"  üìà –¢–µ–≥ '{tag}': {total} —É–ø., –ø–æ–∑={positive_pct:.1f}%, –Ω–µ–≥={negative_pct:.1f}%, –Ω–µ–π—Ç—Ä={neutral_pct:.1f}%")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ (–±–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏)
        signal_type = None
        title = f'–¢—Ä–µ–Ω–¥: {tag}'
        description = f'–ê–∫—Ç–∏–≤–Ω–∞—è —Ç–µ–º–∞: {total} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π'
        icon = 'fas fa-chart-line'
        priority = 50
        trend = 'stable'
        
        if negative_pct > 40 and total >= 5:  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥ —Å 50% –¥–æ 40%
            signal_type = 'problem'
            title = f'–ü—Ä–æ–±–ª–µ–º–∞: {tag}'
            description = f'–ü—Ä–µ–æ–±–ª–∞–¥–∞–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤ ({negative_pct:.1f}% –∏–∑ {total} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)'
            icon = 'fas fa-exclamation-triangle'
            priority = 70 + min(20, negative_pct // 5)
            trend = 'up' if negative_pct > 50 else 'stable'
        elif positive_pct > 40 and total >= 5:  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥ —Å 50% –¥–æ 40%
            signal_type = 'opportunity'
            title = f'–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å: {tag}'
            description = f'–ü—Ä–µ–æ–±–ª–∞–¥–∞–µ—Ç –ø–æ–∑–∏—Ç–∏–≤ ({positive_pct:.1f}% –∏–∑ {total} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)'
            icon = 'fas fa-lightbulb'
            priority = 65 + min(20, positive_pct // 5)
            trend = 'up' if positive_pct > 50 else 'stable'
        elif total >= 5:  # –ü–æ–ø—É–ª—è—Ä–Ω—ã–π —Ç–µ–≥
            signal_type = 'early_trend'
            title = f'–¢—Ä–µ–Ω–¥: {tag}'
            description = f'–ü–æ–ø—É–ª—è—Ä–Ω–∞—è —Ç–µ–º–∞: {total} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π'
            icon = 'fas fa-chart-line'
            priority = 50 + min(10, total // 2)
        
        if not signal_type:
            continue
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—ã
        dates = stats.get('dates', [])
        if dates:
            dates.sort()
            first_seen = dates[0][:10] if dates[0] else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            last_seen = dates[-1][:10] if dates[-1] else '–Ω–µ–¥–∞–≤–Ω–æ'
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã –≤ —Ä—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
            try:
                if first_seen != '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ':
                    dt = datetime.strptime(first_seen, '%Y-%m-%d')
                    first_seen = dt.strftime('%d.%m.%Y')
                if last_seen != '–Ω–µ–¥–∞–≤–Ω–æ':
                    dt = datetime.strptime(last_seen, '%Y-%m-%d')
                    last_seen = dt.strftime('%d.%m.%Y')
            except:
                pass
        else:
            first_seen = '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            last_seen = '–Ω–µ–¥–∞–≤–Ω–æ'
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if signal_type == 'problem':
            recommendations.append(f'–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω—ã –Ω–µ–≥–∞—Ç–∏–≤–∞ –ø–æ —Ç–µ–º–µ "{tag}"')
            recommendations.append('–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø–ª–∞–Ω –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏')
        elif signal_type == 'opportunity':
            recommendations.append(f'–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–Ω–¥ –ø–æ —Ç–µ–º–µ "{tag}"')
            recommendations.append('–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è')
        else:
            recommendations.append(f'–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç–µ–º—ã "{tag}"')
        
        formatted_signals.append({
            'id': signal_id,
            'title': title,
            'description': description,
            'type': signal_type,
            'icon': icon,
            'mentions': total,
            'trend': trend,
            'tag': tag,
            'sentiment': {
                'positive': round(positive_pct, 1),
                'negative': round(negative_pct, 1),
                'neutral': round(neutral_pct, 1)
            },
            'recommendations': recommendations,
            'priority': priority,
            'first_seen': first_seen,
            'last_seen': last_seen,
            'debug': {
                'raw_total': total,
                'raw_positive': stats['positive'],
                'raw_negative': stats['negative'],
                'raw_neutral': stats['neutral']
            }
        })
        signal_id += 1
        
        if len(formatted_signals) >= 6:  # –î–æ 6 —Å–∏–≥–Ω–∞–ª–æ–≤
            break
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    formatted_signals.sort(key=lambda x: x['priority'], reverse=True)
    
    print(f"üìà –°–æ–∑–¥–∞–Ω–æ –±–∞–∑–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {len(formatted_signals)}")
    
    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Å–∏–≥–Ω–∞–ª–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–±—â–∏–µ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è–º
    if not formatted_signals:
        print("‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ–≥–æ–≤, —Å–æ–∑–¥–∞–µ–º –æ–±—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã...")
        cursor = conn.execute('''
        SELECT sentiment, COUNT(*) as count
        FROM chunks 
        WHERE sentiment IS NOT NULL
        GROUP BY sentiment
        ''')
        
        sentiments = cursor.fetchall()
        
        for row in sentiments:
            sentiment = row['sentiment']
            count = row['count']
            
            if count < 5:
                continue
                
            signal_types = {
                'negative': {
                    'type': 'problem',
                    'title': '–û–±—â–∏–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ–Ω',
                    'icon': 'fas fa-exclamation-triangle',
                    'description': f'–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π ({count})',
                    'tag': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ–Ω',
                    'priority': 60
                },
                'positive': {
                    'type': 'opportunity',
                    'title': '–û–±—â–∏–π –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ñ–æ–Ω',
                    'icon': 'fas fa-lightbulb',
                    'description': f'–ü—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ ({count})',
                    'tag': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ñ–æ–Ω',
                    'priority': 55
                },
                'neutral': {
                    'type': 'early_trend',
                    'title': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ñ–æ–Ω',
                    'icon': 'fas fa-chart-line',
                    'description': f'–ú–Ω–æ–≥–æ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π ({count})',
                    'tag': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ñ–æ–Ω',
                    'priority': 50
                }
            }
            
            if sentiment in signal_types:
                signal_info = signal_types[sentiment]
                
                sentiment_dist = {'positive': 0, 'negative': 0, 'neutral': 0}
                sentiment_dist[sentiment] = 100
                
                formatted_signals.append({
                    'id': signal_id,
                    'title': title,
                    'description': description,
                    'type': signal_type,
                    'icon': icon,
                    'mentions': total,
                    'trend': trend,
                    'tag': tag,
                    'sentiment': {
                        'positive': round(positive_pct, 1),
                        'negative': round(negative_pct, 1),
                        'neutral': round(neutral_pct, 1)
                    },
                    # –î–æ–±–∞–≤—å—Ç–µ –≤—ã–∑–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —á–µ—Ä–µ–∑ LLM
                    'recommendations': self._get_llm_recommendations(tag, signal_type, positive_pct, negative_pct, total),
                    'priority': priority,
                    'first_seen': first_seen,
                    'last_seen': last_seen,
                    'debug': {
                        'raw_total': total,
                        'raw_positive': stats['positive'],
                        'raw_negative': stats['negative'],
                        'raw_neutral': stats['neutral']
                    }
                })
                signal_id += 1
        
        # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º 2 –æ–±—â–∏—Ö —Å–∏–≥–Ω–∞–ª–∞
        formatted_signals = formatted_signals[:2]
    
    return formatted_signals[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 —Å–∏–≥–Ω–∞–ª–∞

def get_sentiment_distribution() -> Dict[str, int]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    conn = get_db_connection()
    
    try:
        cursor = conn.execute('''
        SELECT sentiment, COUNT(*) as count
        FROM chunks 
        WHERE sentiment IS NOT NULL
        GROUP BY sentiment
        ''')
        
        distribution = {}
        for row in cursor.fetchall():
            distribution[row['sentiment']] = row['count']
        
        return distribution
        
    finally:
        conn.close()

def get_top_tags(limit: int = 20) -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø —Ç–µ–≥–æ–≤"""
    conn = get_db_connection()
    
    try:
        cursor = conn.execute("SELECT llm_tags FROM chunks WHERE llm_tags IS NOT NULL")
        all_tags = []
        
        for row in cursor.fetchall():
            try:
                tags = json.loads(row['llm_tags'])
                all_tags.extend(tags)
            except:
                pass
        
        tag_counter = Counter(all_tags)
        top_tags = [{'tag': tag, 'count': count} 
                   for tag, count in tag_counter.most_common(limit)]
        
        return top_tags
        
    finally:
        conn.close()

def get_tags_info() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–≥–∞—Ö"""
    conn = get_db_connection()
    
    try:
        cursor = conn.execute("SELECT llm_tags FROM chunks WHERE llm_tags IS NOT NULL")
        all_tags = []
        
        for row in cursor.fetchall():
            try:
                tags = json.loads(row['llm_tags'])
                all_tags.extend(tags)
            except:
                pass
        
        unique_tags = list(set(all_tags))
        
        return {
            'total_tags': len(all_tags),
            'unique_tags': len(unique_tags),
            'tags_list': unique_tags[:50]
        }
        
    finally:
        conn.close()

def _get_llm_recommendations(self, tag: str, signal_type: str, 
                            positive_pct: float, negative_pct: float, 
                            total: int) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç LLM"""
    try:
        from classifier import LLMMetadataClassifier
        classifier = LLMMetadataClassifier(api_key=os.getenv("GROQ_API_KEY"))
        
        sentiment_distribution = {
            'positive': round(positive_pct, 1),
            'negative': round(negative_pct, 1),
            'neutral': round(100 - positive_pct - negative_pct, 1)
        }
        
        trend = 'up' if positive_pct > negative_pct else 'down'
        
        recommendations = classifier.generate_recommendations(
            tag=tag,
            signal_type=signal_type,
            sentiment_distribution=sentiment_distribution,
            mentions_count=total,
            trend=trend
        )
        
        return recommendations[:3] if recommendations else []
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç LLM: {e}")
        return [f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º—É '{tag}'", "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"]