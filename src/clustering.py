import numpy as np
from collections import Counter
from typing import List, Dict
from sklearn.cluster import DBSCAN
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False

class Clusterer:
    def __init__(self, cfg, llm=None):
        self.cfg = cfg
        self.llm = llm
        self.algorithm = cfg["clustering"]["algorithm"]

    def cluster(self, chunks: List[Dict]) -> List[Dict]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        print("–ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø")
        print(f"–ê–ª–≥–æ—Ä–∏—Ç–º: {self.algorithm}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        embs = np.array([c["n_embedding"] for c in chunks])
        
        if self.algorithm == "hdbscan" and HDBSCAN_AVAILABLE:
            min_cluster_size = self.cfg["clustering"]["hdbscan_min_cluster_size"]
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                metric='euclidean'
            )
            labels = clusterer.fit_predict(embs)
        elif self.algorithm == "dbscan" or not HDBSCAN_AVAILABLE:
            eps = self.cfg["clustering"]["dbscan_eps"]
            min_samples = self.cfg["clustering"]["dbscan_min_samples"]
            clusterer = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
            labels = clusterer.fit_predict(embs)
        else:
            raise ValueError(f"Unknown algorithm: {self.algorithm}")

        # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º cluster_id –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
        for i, c in enumerate(chunks):
            c["cluster_id"] = int(labels[i])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        unique_clusters = set(labels)
        noise_count = sum(1 for l in labels if l == -1)
        
        print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
        print(f"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(unique_clusters) - (1 if -1 in unique_clusters else 0)}")
        print(f"  ‚Ä¢ –®—É–º (–Ω–µ –≤–æ—à–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã): {noise_count} —á–∞–Ω–∫–æ–≤")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        cluster_counts = Counter(labels)
        print(f"\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
        for cluster_id in sorted(cluster_counts.keys()):
            if cluster_id != -1:
                count = cluster_counts[cluster_id]
                percentage = (count / len(chunks)) * 100
                print(f"  ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id:2d}: {count:3d} —á–∞–Ω–∫–æ–≤ ({percentage:5.1f}%)")
        
        if noise_count > 0:
            percentage = (noise_count / len(chunks)) * 100
            print(f"  ‚Ä¢ –®—É–º      -1: {noise_count:3d} —á–∞–Ω–∫–æ–≤ ({percentage:5.1f}%)")
        
        
        return chunks

    def name_clusters(self, chunks: List[Dict]) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        """
        print("–ò–ú–ï–ù–û–í–ê–ù–ò–ï –ö–õ–ê–°–¢–ï–†–û–í")
        
        clusters_map = {}
        for c in chunks:
            cid = c["cluster_id"]
            if cid not in clusters_map:
                clusters_map[cid] = []
            clusters_map[cid].append(c)

        result = {}
        
        for cid, cluster_chunks in clusters_map.items():
            print(f"\nüî∏ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cid} ({len(cluster_chunks)} —á–∞–Ω–∫–æ–≤)...")
            
            if cid == -1:
                # –®—É–º
                result[str(cid)] = {
                    "name_short": "–®—É–º / –†–∞–∑–Ω–æ–µ",
                    "name_long": "–ß–∞–Ω–∫–∏, –Ω–µ –ø–æ–ø–∞–≤—à–∏–µ –Ω–∏ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä",
                    "size": len(cluster_chunks),
                    "top_tags": []
                }
                print("  ‚úì –ö–ª–∞—Å—Ç–µ—Ä —à—É–º–∞")
                continue

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–≥–∏ –∏–∑ —á–∞–Ω–∫–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞
            all_tags = []
            for ch in cluster_chunks:
                tags = ch.get("meta", {}).get("tags", [])
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ —Å—Ç—Ä–æ–∫ –∏ —Å–ø–∏—Å–∫–æ–≤
                if isinstance(tags, str):
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π
                    tags = [t.strip() for t in tags.split(",") if t.strip()]
                elif isinstance(tags, list):
                    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    tags = [str(t).strip() for t in tags if t]
                else:
                    # –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    tags = []
                
                all_tags.extend(tags)

            # –¢–æ–ø-5 —Ç–µ–≥–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            tag_counts = Counter(all_tags)
            top_tags = [tag for tag, _ in tag_counts.most_common(5)]

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            categories = []
            sentiments = []
            for ch in cluster_chunks:
                meta = ch.get("meta", {})
                if "category" in meta:
                    categories.append(meta["category"])
                if "sentiment" in meta:
                    sentiments.append(meta["sentiment"])

            # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            if categories:
                main_category = Counter(categories).most_common(1)[0][0]
            else:
                main_category = "–æ–±—â–µ–µ"
            
            # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
            if sentiments:
                main_sentiment = Counter(sentiments).most_common(1)[0][0]
            else:
                main_sentiment = "neutral"

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ LLM (–µ—Å–ª–∏ real mode)
            if self.llm and self.llm.mode == "real":
                sample_texts = [ch["text"][:200] for ch in cluster_chunks[:3]]
                prompt = f"–î–∞–π –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (2-4 —Å–ª–æ–≤–∞) –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:\n\n"
                prompt += "\n---\n".join(sample_texts)
                prompt += f"\n\n–¢–µ–≥–∏: {', '.join(top_tags[:5])}\n–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {main_category}"
                
                try:
                    name_short = self.llm.generate(prompt).strip()
                except:
                    name_short = f"{main_category.capitalize()}"
            else:
                # Mock: –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ —Ç–æ–ø —Ç–µ–≥
                if top_tags:
                    name_short = f"{main_category.capitalize()}: {top_tags[0]}"
                else:
                    name_short = main_category.capitalize()

            result[str(cid)] = {
                "name_short": name_short,
                "name_long": f"–ö–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(cluster_chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ —Ç–µ–º–µ '{main_category}'",
                "size": len(cluster_chunks),
                "top_tags": top_tags,
                "main_category": main_category,
                "main_sentiment": main_sentiment,
                "sentiment_distribution": dict(Counter(sentiments)) if sentiments else {}
            }
            
            print(f"  ‚úì {name_short}")
            print(f"    –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {main_category}")
            print(f"    –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {main_sentiment}")
            print(f"    –¢–æ–ø —Ç–µ–≥–∏: {', '.join(top_tags[:3])}")

        print(f" –ò–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len([k for k in result.keys() if k != '-1'])} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

        return result

    def get_cluster_summary(self, cluster_id: int, chunks: List[Dict]) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É
        """
        cluster_chunks = [c for c in chunks if c.get("cluster_id") == cluster_id]
        
        if not cluster_chunks:
            return {"error": "cluster_not_found"}
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        all_tags = []
        categories = []
        sentiments = []
        
        for ch in cluster_chunks:
            meta = ch.get("meta", {})
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–≥–æ–≤ (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ø–∏—Å–æ–∫)
            tags = meta.get("tags", [])
            if isinstance(tags, str):
                tags = [t.strip() for t in tags.split(",") if t.strip()]
            elif isinstance(tags, list):
                tags = [str(t).strip() for t in tags if t]
            
            all_tags.extend(tags)
            
            if "category" in meta:
                categories.append(meta["category"])
            if "sentiment" in meta:
                sentiments.append(meta["sentiment"])
        
        return {
            "cluster_id": cluster_id,
            "size": len(cluster_chunks),
            "top_tags": [tag for tag, _ in Counter(all_tags).most_common(10)],
            "categories": dict(Counter(categories)),
            "sentiments": dict(Counter(sentiments)),
            "sample_texts": [ch["text"][:150] + "..." for ch in cluster_chunks[:3]]
        }