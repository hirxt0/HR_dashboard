# test.py
import os
from classifier import LLMMetadataClassifier

# –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
TEST_TEXTS = [
    """–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ—Ç—Ä–∞—Å–ª–∏. 
    –ù–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–≥—Ä–æ–º–Ω—ã–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö 
    –∏ –¥–µ–ª–∞—Ç—å —Ç–æ—á–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã. –ö—Ä—É–ø–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ä—É—é—Ç –º–∏–ª–ª–∏–∞—Ä–¥—ã –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –ò–ò, 
    —á—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π.""",
    
    """–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä—ã–Ω–∫–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ñ–æ–Ω–µ –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π 
    –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏. –ò–Ω–≤–µ—Å—Ç–æ—Ä—ã –ø—Ä–æ—è–≤–ª—è—é—Ç –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å, —á—Ç–æ —Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è—Ö 
    –≤ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å–µ–∫—Ç–æ—Ä. –≠–∫—Å–ø–µ—Ä—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω—ã–π –∫—Ä–∏–∑–∏—Å –≤ –±–ª–∏–∂–∞–π—à–∏–µ –º–µ—Å—è—Ü—ã.""",
    
    """–ù–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—é –º–µ–Ω—è—é—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ. 
    –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã –≤–Ω–µ–¥—Ä—è—é—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞–º 
    —Å–æ –≤—Å–µ–≥–æ –º–∏—Ä–∞ –ø–æ–ª—É—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã 
    –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –Ω–∞—É–∫–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.""",
    
    """–ú–æ–ª–æ–¥—ã–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞—é—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç–∞—Ä—Ç–∞–ø—ã –≤ —Å—Ñ–µ—Ä–µ 
    —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –º–Ω–æ–≥–∏–µ 
    –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞—Ö–æ–¥—è—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É —É –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.""",
]

def test_without_db():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –±–µ–∑ –ë–î"""
    
    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        print("‚ùå –û—à–∏–±–∫–∞: GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        print("üìù –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º: GROQ_API_KEY=–≤–∞—à_–∫–ª—é—á")
        print("üí° –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è: export GROQ_API_KEY=–≤–∞—à_–∫–ª—é—á")
        return
    
    print("=" * 70)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –° LLM")
    print("üìå LLM –≤—ã–±–∏—Ä–∞–µ—Ç —Ç–µ–≥–∏ –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–µ–∑ –ë–î
    classifier = LLMMetadataClassifier(db_path=None, api_key=api_key)
    
    # –í—ã–≤–æ–¥–∏–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ–≥–∏
    print(f"\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ–≥–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ ({len(classifier.PREDEFINED_TAGS)}):")
    print(", ".join(classifier.PREDEFINED_TAGS[:15]) + "...")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
    for i, text in enumerate(TEST_TEXTS, 1):
        print(f"\n{'='*70}")
        print(f"üß™ –¢–ï–°–¢ {i}")
        print(f"{'='*70}")
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            result = classifier.analyze_text(text)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if i < len(TEST_TEXTS):
            print("\n" + "-"*50)
            input("‚è∏Ô∏è  –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")

def test_with_custom_text():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        print("‚ùå –û—à–∏–±–∫–∞: GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    classifier = LLMMetadataClassifier(db_path=None, api_key=api_key)
    
    print("\n" + "="*70)
    print("‚úçÔ∏è  –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –í–ê–®–ò–ú –¢–ï–ö–°–¢–û–ú")
    print("="*70)
    
    while True:
        print("\nüìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):")
        user_text = input("> ")
        
        if user_text.lower() == 'exit':
            break
        
        if len(user_text.strip()) < 10:
            print("‚ö†Ô∏è  –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π. –ú–∏–Ω–∏–º—É–º 10 —Å–∏–º–≤–æ–ª–æ–≤.")
            continue
        
        try:
            result = classifier.analyze_text(user_text)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")

def create_test_db():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –ë–î –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏"""
    import sqlite3
    import json
    
    print("\nüóÑÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –ë–î...")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –ë–î –µ—Å–ª–∏ –µ—Å—Ç—å
    if os.path.exists("test_telegram_data.db"):
        os.remove("test_telegram_data.db")
    
    # –°–æ–∑–¥–∞–µ–º –ë–î –∏ —Ç–∞–±–ª–∏—Ü—É
    conn = sqlite3.connect("test_telegram_data.db")
    cursor = conn.cursor()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É chunks
    cursor.execute("""
    CREATE TABLE chunks (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        text TEXT NOT NULL,
        metadata TEXT,
        llm_tags TEXT,
        sentiment TEXT,
        explanation TEXT
    )
    """)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    for i, text in enumerate(TEST_TEXTS, 1):
        cursor.execute("""
        INSERT INTO chunks (text, metadata) 
        VALUES (?, ?)
        """, (
            text,
            json.dumps({"source": "test", "id": i})
        ))
    
    conn.commit()
    conn.close()
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Ç–µ—Å—Ç–æ–≤–∞—è –ë–î: test_telegram_data.db")
    print(f"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ {len(TEST_TEXTS)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    return "test_telegram_data.db"

def test_with_db(db_path, limit=None):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ë–î"""
    import sqlite3
    
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key or api_key == "–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å":
        print("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        return
    
    print(f"\nüóÑÔ∏è  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ë–î: {db_path}")
    print(f"üìä –õ–∏–º–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {'–≤—Å–µ' if limit is None else limit} –∑–∞–ø–∏—Å–µ–π")
    
    classifier = LLMMetadataClassifier(
        db_path=db_path, 
        api_key=api_key
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –≤ –ë–î
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM chunks")
    total_count = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM chunks WHERE llm_tags IS NOT NULL")
    processed_count = cursor.fetchone()[0]
    conn.close()
    
    print(f"üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count}")
    print(f"‚úÖ –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}")
    print(f"‚è≥ –û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {total_count - processed_count}")
    
    results = classifier.process_all(limit=limit, delay=1.0)
    
    if results:
        print(f"\nüéâ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        for result in results:
            print(f"\nüß© –ß–∞–Ω–∫ {result['chunk_id']}:")
            print(f"   üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(result['llm_tags'])}")
            print(f"   üòä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {result['sentiment']}")
            print(f"   üìù –û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {result['explanation'][:100]}...")
        
        return results
    else:
        print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø–∏—Å–∏")
        return []

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª .env –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not os.path.exists(".env"):
        with open(".env", "w") as f:
            f.write("# –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à API –∫–ª—é—á Groq\n")
            f.write("# –ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –Ω–∞ https://console.groq.com/api-keys\n")
            f.write("GROQ_API_KEY=–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å\n")
        print("üìù –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª .env. –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à GROQ_API_KEY –≤ —ç—Ç–æ—Ç —Ñ–∞–π–ª.")
        return
    
    # –ß–∏—Ç–∞–µ–º API –∫–ª—é—á
    from dotenv import load_dotenv
    load_dotenv()
    
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key or api_key == "–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å":
        print("‚ùå –û—à–∏–±–∫–∞: GROQ_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        print("üìù –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª .env –∏ –∑–∞–º–µ–Ω–∏—Ç–µ '–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å' –Ω–∞ –≤–∞—à –Ω–∞—Å—Ç–æ—è—â–∏–π API –∫–ª—é—á")
        return
    
    print("=" * 70)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –ú–ï–¢–ê–î–ê–ù–ù–´–•")
    print("üìå –¢–µ–≥–∏ + –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ + –û–±—ä—è—Å–Ω–µ–Ω–∏–µ")
    print("=" * 70)
    
    test_db_path = None
    
    while True:
        print("\nüéÆ –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:")
        print("1. üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö (–±–µ–∑ –ë–î)")
        print("2. ‚úçÔ∏è  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤–∞—à–∏–º —Ç–µ–∫—Å—Ç–æ–º (–±–µ–∑ –ë–î)")
        print("3. üóÑÔ∏è  –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é –ë–î")
        print("4. üîÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–°–ï –∑–∞–ø–∏—Å–∏ –∏–∑ –ë–î")
        print("5. ‚ö° –û–±—Ä–∞–±–æ—Ç–∞—Ç—å 2 –∑–∞–ø–∏—Å–∏ –∏–∑ –ë–î (–±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç)")
        print("6. üìä –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î")
        print("7. üö™ –í—ã—Ö–æ–¥")
        
        choice = input("üëâ –í–∞—à –≤—ã–±–æ—Ä (1-7): ").strip()
        
        if choice == "1":
            test_without_db()
        elif choice == "2":
            test_with_custom_text()
        elif choice == "3":
            test_db_path = create_test_db()
        elif choice == "4":
            if test_db_path or os.path.exists("test_telegram_data.db"):
                db_path = test_db_path or "test_telegram_data.db"
                test_with_db(db_path, limit=None)
            else:
                print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—É—é –ë–î (–æ–ø—Ü–∏—è 3)")
        elif choice == "5":
            if test_db_path or os.path.exists("test_telegram_data.db"):
                db_path = test_db_path or "test_telegram_data.db"
                test_with_db(db_path, limit=2)
            else:
                print("‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—É—é –ë–î (–æ–ø—Ü–∏—è 3)")
        elif choice == "6":
            show_db_stats()
        elif choice == "7":
            print("üëã –í—ã—Ö–æ–¥...")
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

def show_db_stats():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î"""
    import sqlite3
    import json
    
    db_path = "test_telegram_data.db"
    if not os.path.exists(db_path):
        print("‚ö†Ô∏è  –¢–µ—Å—Ç–æ–≤–∞—è –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    cursor.execute("SELECT COUNT(*) FROM chunks")
    total = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM chunks WHERE llm_tags IS NOT NULL")
    processed = cursor.fetchone()[0]
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è–º
    cursor.execute("SELECT sentiment, COUNT(*) FROM chunks WHERE sentiment IS NOT NULL GROUP BY sentiment")
    sentiments = cursor.fetchall()
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î:")
    print(f"üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total}")
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}")
    print(f"‚è≥ –ù–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total - processed}")
    
    if processed > 0:
        print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processed/total*100:.1f}%")
    
    if sentiments:
        print(f"\nüòä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π:")
        for sentiment, count in sentiments:
            print(f"   {sentiment}: {count} ({count/processed*100:.1f}%)")
    
    # –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    cursor.execute("""
        SELECT llm_tags, sentiment, explanation 
        FROM chunks 
        WHERE llm_tags IS NOT NULL 
        LIMIT 3
    """)
    examples = cursor.fetchall()
    
    if examples:
        print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        for i, (tags_json, sentiment, explanation) in enumerate(examples, 1):
            try:
                tags = json.loads(tags_json)
                print(f"\n   –ü—Ä–∏–º–µ—Ä {i}:")
                print(f"      üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(tags)}")
                print(f"      üòä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment}")
                print(f"      üìù –û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {explanation[:80]}...")
            except:
                print(f"\n   –ü—Ä–∏–º–µ—Ä {i}: –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
    
    conn.close()

if __name__ == "__main__":
    main()