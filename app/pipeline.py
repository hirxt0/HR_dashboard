"""
pipeline.py - –°–æ–∑–¥–∞–Ω–∏–µ –ë–î —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –∑–∞–ø—É—Å–∫ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
"""
import os
import sys
import time
from datetime import datetime
import json
import sqlite3

def print_step(step_num, description):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —à–∞–≥–æ–≤"""
    print(f"\n{'='*60}")
    print(f"üìã –®–ê–ü {step_num}: {description}")
    print(f"{'='*60}")

class DataPipeline:
    def __init__(self):
        self.db_path = "telegram_data.db"
        
    def run_full_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê: –°–û–ó–î–ê–ù–ò–ï –ë–ê–ó–´ –î–ê–ù–ù–´–• –ò LLM –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø")
        print("=" * 70)
        
        # –®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ –ë–î —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏
        self.step1_create_realistic_database()
        
        # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM (—Ç–µ–≥–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)
        self.step2_llm_classification()
        
        # –®–∞–≥ 3: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        self.step3_basic_trend_analysis()
        
        print("\n" + "=" * 70)
        print("‚úÖ –ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 70)
        print("\nüìã –ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ:")
        print("1. üìÅ –°–æ–∑–¥–∞–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å 50 —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏")
        print("2. üß† –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ LLM (—Ç–µ–≥–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ)")
        print("3. üìà –ü—Ä–æ–≤–µ–¥–µ–Ω –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤")
        print("\nüöÄ –î–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: python app.py")
        print("üåê –ó–∞—Ç–µ–º –æ—Ç–∫—Ä–æ–π—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ: http://localhost:5000")
    
    def step1_create_realistic_database(self):
        """–®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ –ë–î —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏"""
        print_step(1, "–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏")
        
        from database import init_database, create_realistic_news
        
        print("üóÑÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        init_database()
        
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
        create_realistic_news()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM chunks")
        total = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM chunks WHERE llm_tags IS NOT NULL")
        classified = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM chunks WHERE sentiment IS NOT NULL")
        with_sentiment = cursor.fetchone()[0]
        
        conn.close()
        
        print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {total} –Ω–æ–≤–æ—Å—Ç–µ–π")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total}")
        print(f"   ‚Ä¢ –° —Ç–µ–≥–∞–º–∏ (LLM): {classified}")
        print(f"   ‚Ä¢ –° –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º: {with_sentiment}")
    
    def step2_llm_classification(self):
        """–®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM"""
        print_step(2, "–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è)")
        
        try:
            from classifier import LLMMetadataClassifier
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
            api_key = os.getenv("GROQ_API_KEY")
            if not api_key or api_key == "–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å":
                print("‚ö†Ô∏è  GROQ_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
                print("‚ùå –ë–µ–∑ API –∫–ª—é—á–∞ LLM –Ω–µ —Å–º–æ–∂–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏")
                print("‚ÑπÔ∏è  –ó–∞–ø–æ–ª–Ω–∏—Ç–µ .env —Ñ–∞–π–ª –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω —Å–Ω–æ–≤–∞")
                return
            
            print("üß† –ó–∞–ø—É—Å–∫ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")
            print("‚ÑπÔ∏è  LLM –±—É–¥–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å:")
            print("   ‚Ä¢ 3-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–≥–æ–≤")
            print("   ‚Ä¢ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (positive/neutral/negative)")
            print("   ‚Ä¢ –ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ —Ç–µ–≥–æ–≤")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            classifier = LLMMetadataClassifier(
                db_path=self.db_path,
                api_key=api_key
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ —Ç–µ–≥–æ–≤
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM chunks WHERE llm_tags IS NULL")
            unclassified_count = cursor.fetchone()[0]
            conn.close()
            
            if unclassified_count == 0:
                print("‚úÖ –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ LLM")
                return
            
            print(f"üìù –ù–∞–π–¥–µ–Ω–æ {unclassified_count} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ LLM...")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Ä—Ü–∏—è–º–∏ –ø–æ 10 –Ω–æ–≤–æ—Å—Ç–µ–π
            batch_size = 10
            total_processed = 0
            
            while True:
                chunks = classifier.get_chunks(limit=batch_size)
                if not chunks:
                    break
                
                print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(chunks)} –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–∞–∫–µ—Ç {total_processed//batch_size + 1})...")
                
                results = []
                for i, chunk in enumerate(chunks, 1):
                    try:
                        print(f"   üì∞ –ù–æ–≤–æ—Å—Ç—å {total_processed + i}: {chunk.text[:80]}...")
                        
                        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM
                        llm_tags, sentiment, explanation = classifier.analyze_with_llm(chunk.text)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                        classifier.save_to_db(
                            chunk.chunk_id, 
                            llm_tags, 
                            sentiment, 
                            explanation
                        )
                        
                        results.append({
                            'id': chunk.chunk_id,
                            'tags': llm_tags,
                            'sentiment': sentiment
                        })
                        
                        print(f"     ‚úÖ –¢–µ–≥–∏: {', '.join(llm_tags[:3])}")
                        print(f"     üòä –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment}")
                        
                        # –ó–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å rate limit
                        time.sleep(1.5)
                        
                    except Exception as e:
                        print(f"     ‚ùå –û—à–∏–±–∫–∞: {e}")
                        continue
                
                total_processed += len(chunks)
                print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}/{unclassified_count}")
                
                if len(chunks) < batch_size:
                    break
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT sentiment, COUNT(*) FROM chunks WHERE sentiment IS NOT NULL GROUP BY sentiment")
            sentiment_stats = cursor.fetchall()
            
            cursor.execute('''SELECT COUNT(DISTINCT json_each.value) 
                            FROM chunks, json_each(llm_tags) 
                            WHERE llm_tags IS NOT NULL''')
            unique_tags = cursor.fetchone()[0]
            
            conn.close()
            
            print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
            print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {total_processed}")
            print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤: {unique_tags}")
            print(f"   ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π:")
            for sentiment, count in sentiment_stats:
                print(f"     - {sentiment}: {count}")
            
            print("‚úÖ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            import traceback
            traceback.print_exc()
    
    def step3_basic_trend_analysis(self):
        """–®–∞–≥ 3: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        print_step(3, "–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤")
        
        try:
            print("üìà –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–≥–æ–≤ –∏ —Ç—Ä–µ–Ω–¥–æ–≤...")
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏
            cursor.execute('''
            SELECT json_each.value as tag, COUNT(*) as count
            FROM chunks, json_each(llm_tags)
            WHERE llm_tags IS NOT NULL
            GROUP BY tag
            ORDER BY count DESC
            LIMIT 15
            ''')
            
            popular_tags = cursor.fetchall()
            
            print(f"üè∑Ô∏è  –¢–æ–ø-15 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–≥–æ–≤:")
            for i, (tag, count) in enumerate(popular_tags, 1):
                print(f"   {i:2}. {tag:<25} - {count:2} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è–º
            cursor.execute('''
            SELECT 
                sentiment,
                COUNT(*) as count,
                ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM chunks WHERE sentiment IS NOT NULL), 1) as percentage
            FROM chunks 
            WHERE sentiment IS NOT NULL
            GROUP BY sentiment
            ORDER BY count DESC
            ''')
            
            sentiment_stats = cursor.fetchall()
            
            print(f"\nüòä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π:")
            for sentiment, count, percentage in sentiment_stats:
                print(f"   ‚Ä¢ {sentiment}: {count} –Ω–æ–≤–æ—Å—Ç–µ–π ({percentage}%)")
            
            # –¢—Ä–µ–Ω–¥—ã –ø–æ –¥–∞—Ç–∞–º
            cursor.execute('''
            SELECT 
                strftime('%Y-%m-%d', created_at) as date,
                COUNT(*) as news_count
            FROM chunks
            GROUP BY date
            ORDER BY date DESC
            LIMIT 7
            ''')
            
            recent_dates = cursor.fetchall()
            
            print(f"\nüìÖ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π:")
            for date, count in recent_dates:
                print(f"   ‚Ä¢ {date}: {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
            
            conn.close()
            
            print("‚úÖ –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ HR ANALYTICS DASHBOARD - –°–û–ó–î–ê–ù–ò–ï –ë–ê–ó–´ –î–ê–ù–ù–´–•")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞–µ–º .env –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not os.path.exists('.env'):
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ .env...")
        with open('.env', 'w', encoding='utf-8') as f:
            f.write("""# API –∫–ª—é—á–∏ –¥–ª—è HR Analytics Dashboard
GROQ_API_KEY=–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å
GIGACHAT_API_KEY=–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DEBUG=True
PORT=5000
""")
        print("‚úÖ –§–∞–π–ª .env —Å–æ–∑–¥–∞–Ω")
        print("‚ö†Ô∏è  –ó–ê–ü–û–õ–ù–ò–¢–ï API –ö–õ–Æ–ß –í –§–ê–ô–õ–ï .env –ü–ï–†–ï–î –ó–ê–ü–£–°–ö–û–ú!")
        print("   –ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –Ω–∞: https://console.groq.com/keys")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
    with open('.env', 'r') as f:
        content = f.read()
        if '–≤–∞—à_–∫–ª—é—á_–∑–¥–µ—Å—å' in content:
            print("‚ùå –í–ê–ñ–ù–û: –ó–∞–ø–æ–ª–Ω–∏—Ç–µ GROQ_API_KEY –≤ —Ñ–∞–π–ª–µ .env!")
            print("   –ë–µ–∑ API –∫–ª—é—á–∞ LLM –Ω–µ —Å–º–æ–∂–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏")
            return
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = DataPipeline()
    pipeline.run_full_pipeline()

if __name__ == "__main__":
    main()